{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bc8786-3ea7-4b21-a53f-09675d86534b",
   "metadata": {},
   "source": [
    "# Introduction to Snowpark pandas\n",
    "The Snowpark pandas API allows you to run your pandas code directly on your data in Snowflake. Built to replicate the functionality of pandas - including its data isolation and consistency guarantees - the Snowpark pandas API enables you to scale up your traditional pandas pipelines with just a few lines of change.\n",
    "\n",
    "In today's demo, we'll be taking a look at how you can get started with the API, as well as comparing its simpilarity with vanilla pandas.\n",
    "\n",
    "## Importing Snowpark pandas\n",
    "Much like Snowpark, Snowpark pandas requires an active `Session` object to connect to your data in Snowflake. In the next cell, we'll be initializing a Session object, and importing both Snowpark pandas and vanilla pandas, as `spd` and `pd` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9388c10-9876-47a2-82a6-da35d120ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark.modin.pandas as spd\n",
    "import pandas as pd\n",
    "import json\n",
    "from snowflake.snowpark.session import Session\n",
    "# Create Snowflake Session object\n",
    "# connection_parameters = json.load(open('creds.json'))\n",
    "session = Session.builder.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2fb71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the new warehouse time to start.\n",
    "spd.DataFrame([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3812257-0a82-43a0-aaac-d00681558890",
   "metadata": {},
   "source": [
    "## Getting Started - Reading Data from Snowflake\n",
    "Today, we'll be analyzing some Stock Timeseries Data from Snowflake's Marketplace. The data is available courtesy of Cybersyn Inc., and can be found [here](https://app.snowflake.com/marketplace/listing/GZTSZAS2KF7/cybersyn-inc-financial-economic-essentials). Let's start by reading the `stock_price_timeseries` table into a DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03298234-aabe-4548-99b1-bfdb609bdafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into a Snowpark pandas df \n",
    "from time import perf_counter\n",
    "start = perf_counter()\n",
    "spd_df = spd.read_snowflake(\"FINANCIAL__ECONOMIC_ESSENTIALS.CYBERSYN.STOCK_PRICE_TIMESERIES\")\n",
    "end = perf_counter()\n",
    "data_size = len(spd_df)\n",
    "print(f\"Took {end - start} seconds to read a table with {data_size} rows into Snowpark pandas!\")\n",
    "snow_time = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ac2fd-c636-4bb5-a27d-58a5e4cbea7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read data into a local Vanilla pandas df - recommended to kill this cell after waiting a few minutes!\n",
    "# Create a cursor object.\n",
    "from IPython import display\n",
    "start = perf_counter()\n",
    "\n",
    "# Create a cursor object.\n",
    "cur = session.connection.cursor()\n",
    "\n",
    "# Execute a statement that will generate a result set.\n",
    "sql = \"select * from FINANCIAL__ECONOMIC_ESSENTIALS.CYBERSYN.STOCK_PRICE_TIMESERIES\"\n",
    "cur.execute(sql)\n",
    "\n",
    "# Fetch the result set from the cursor and deliver it as the pandas DataFrame.\n",
    "native_pd_df = cur.fetch_pandas_all()\n",
    "end = perf_counter()\n",
    "print(f\"Vanilla pandas took {end - start} seconds to read the data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72085630-11c4-4df1-9627-7c50c0957906",
   "metadata": {},
   "source": [
    "As we can see, pandas is not able to pull the data into memory even after being given a few minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c3dee5-62fe-46c6-a216-133a0140222d",
   "metadata": {},
   "source": [
    "## Examine The Raw Data\n",
    "Let's take a look at the data we're going to be working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a623bed-aed9-4cdb-a3c8-33e9e7da52af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spd_df.head(5).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba83c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "native_pd_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822e6ae-9810-4ca1-8646-660eb3e68d97",
   "metadata": {},
   "source": [
    "## Filtering The Data\n",
    "Let's take a look at some common data transformations - starting with filtering! Let's filter for stocks that are listed on the New York Stock Exchange!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4218fceb-68f1-41be-8c08-3f6ad51424d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "nyse_spd_df = spd_df[(spd_df['PRIMARY_EXCHANGE_CODE'] == 'NYS')]\n",
    "repr(nyse_spd_df)\n",
    "end = perf_counter()\n",
    "print(f\"Filtering for stocks belonging to the NYSE took {end - start} seconds in Snowpark pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e992e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "nyse_native_df = native_pd_df[(native_pd_df['PRIMARY_EXCHANGE_CODE'] == 'NYS')]\n",
    "repr(nyse_native_df)\n",
    "end = perf_counter()\n",
    "print(f\"Filtering for stocks belonging to the NYSE took {end - start} seconds in native pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e325b5-8e24-4dbc-93da-c2e93d84590f",
   "metadata": {},
   "source": [
    "Let's try an even more granular filter - let's filter for the Pre-Market Open of stocks that have the following tickers:\n",
    "* GOOG (Alphabet, Inc.)\n",
    "* MSFT (Microsoft)\n",
    "* SNOW (Snowflake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d456c29-7689-4599-bcd6-02c646ef8f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "filtered_spd_df = spd_df[((spd_df['TICKER'] == 'GOOG') | (spd_df['TICKER'] == 'MSFT') | (spd_df['TICKER'] == 'SNOW')) & (spd_df['VARIABLE_NAME'] == 'Pre-Market Open')]\n",
    "repr(filtered_spd_df)\n",
    "end = perf_counter()\n",
    "print(f\"Filtering for the Pre-Market Open price for the above stocks belonging took {end - start} seconds in Snowpark pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "filtered_native_df = native_pd_df[((native_pd_df['TICKER'] == 'GOOG') | (native_pd_df['TICKER'] == 'MSFT') | (native_pd_df['TICKER'] == 'SNOW')) & (native_pd_df['VARIABLE_NAME'] == 'Pre-Market Open')]\n",
    "repr(filtered_native_df)\n",
    "end = perf_counter()\n",
    "print(f\"Filtering for the Pre-Market Open price for the above stocks belonging took {end - start} seconds in Snowpark pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5cb07-ccce-481e-b41e-770d4de91b0f",
   "metadata": {},
   "source": [
    "# Reshaping the Data\n",
    "Let's say we wanted to analyse the performance of various stock prices across time - in that case, it may be more helpful to have the values as columns, and the ticker name and date as the index - rather than the current encoding. We can accomplish this using the `pivot_table` API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8f893a-c7dc-4e08-bace-3c93ada282cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "reshape_df = spd_df.pivot_table(index=[\"TICKER\", \"DATE\"], columns=\"VARIABLE_NAME\", values=\"VALUE\")\n",
    "repr(reshape_df)\n",
    "end = perf_counter()\n",
    "print(f\"Pivoting the DataFrame took {end - start} seconds in Snowpark pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0b9d1-a3be-4d05-9481-f54628f3b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_df.head(5).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "reshape_native_df = native_pd_df.pivot_table(index=[\"TICKER\", \"DATE\"], columns=\"VARIABLE_NAME\", values=\"VALUE\")\n",
    "repr(reshape_native_df)\n",
    "end = perf_counter()\n",
    "print(f\"Pivoting the DataFrame took {end - start} seconds in native pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3121c325-d7cf-47d6-a2d7-e759ece59d11",
   "metadata": {},
   "source": [
    "## Transforming the Data\n",
    "Now that we have reformatted the data, we can beginn to apply some transformations. Let's start by taking a look at the All-Day Low column for the tickers above - we can resample the data to look at the Quarterly Low for the `GOOG` ticker!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06f23b-12dc-4387-bb87-bc4cbcff6a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "resampled_spd_df_all_quarter_low = reshape_df[\"All-Day Low\"][\"GOOG\"].resample(\"91D\").min()\n",
    "repr(resampled_spd_df_all_quarter_low)\n",
    "end = perf_counter()\n",
    "print(f\"Resampling the DataFrame took {end - start} seconds in Snowpark pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978f55a-c28a-4b7f-9f20-4a2952d2a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_spd_df_all_quarter_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_goog_frame = reshape_native_df[\"All-Day Low\"][\"GOOG\"]\n",
    "# native pandas checks that index is of date type. snowpark pandas does not.\n",
    "pandas_goog_frame.index = pandas_goog_frame.index.astype('datetime64[ns]')\n",
    "start = perf_counter()\n",
    "resampled_native_df_all_quarter_low = pandas_goog_frame.resample(\"91D\").min()\n",
    "repr(resampled_native_df_all_quarter_low)\n",
    "end = perf_counter()\n",
    "print(f\"Resampling the DataFrame took {end - start} seconds in native pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c512e74d-7316-44de-a644-917270d38fac",
   "metadata": {},
   "source": [
    "We can even take a look at the quarter-over-quarter fluctuation in prices using the `diff` API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb467dd6-cc74-423f-b17b-46541f5bbff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "q_o_q_resampled_spd_df_all_quarter_low = resampled_spd_df_all_quarter_low.diff()\n",
    "repr(q_o_q_resampled_spd_df_all_quarter_low)\n",
    "end = perf_counter()\n",
    "print(f\"diffing the DataFrame took {end - start} seconds in Snowpark pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866628d5-5bf9-4212-bba2-bf5e816a70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_o_q_resampled_spd_df_all_quarter_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d8410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "q_o_q_resampled_native_df_all_quarter_low = resampled_native_df_all_quarter_low.diff()\n",
    "repr(q_o_q_resampled_native_df_all_quarter_low)\n",
    "end = perf_counter()\n",
    "print(f\"diffing the DataFrame took {end - start} seconds in native pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dd0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_o_q_resampled_native_df_all_quarter_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7593697-feb5-40a7-9d6c-7c011ad35186",
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Markdown(data=f\"\"\"## Conclusion\\nAs we can see, Snowpark pandas is able to replicate the pandas API while performing computations on large data sets that don't typically work with vanilla pandas and all while keeping your data in Snowflake!\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d385589-1f03-4603-b2a1-54f144c20f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e9483a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
