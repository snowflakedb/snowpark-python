{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Advanced Hybrid Snowpark Pandas Testing Notebook\n",
        "\n",
        "This notebook demonstrates advanced hybrid execution capabilities in Snowpark pandas, focusing on complex transformations and cross-backend operations.\n",
        "\n",
        "## Key Features Tested:\n",
        "- Complex transformations (window functions, pivoting, reshaping)\n",
        "- Cross-backend operations (joins, merges, concatenation)\n",
        "- Advanced analytics with mixed backends\n",
        "- Data movement optimization scenarios\n",
        "- Performance comparison across backends\n",
        "- Environment: **hybrid-pandas-dogfood-1.34.0-python-3.12**\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Environment Verification ===\n",
            "Snowpark version: 1.34.0\n",
            "Hybrid execution environment: hybrid-pandas-dogfood-1.34.0-python-3.12\n",
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Environment: hybrid-pandas-dogfood-1.34.0-python-3.12\n",
        "# Import required libraries for hybrid execution\n",
        "import modin.pandas as pd\n",
        "import snowflake.snowpark.modin.plugin\n",
        "from snowflake.snowpark.session import Session\n",
        "from modin.config import AutoSwitchBackend\n",
        "import snowflake.snowpark as snowpark\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Verify versions and hybrid capabilities\n",
        "print(\"=== Environment Verification ===\")\n",
        "print(f\"Snowpark version: {snowpark.__version__}\")\n",
        "print(f\"Hybrid execution environment: hybrid-pandas-dogfood-1.34.0-python-3.12\")\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to Snowflake: \"sfctest0\"\n",
            "Hybrid execution enabled: True\n",
            "✓ Basic hybrid execution verified\n"
          ]
        }
      ],
      "source": [
        "# Connect to Snowflake and enable hybrid execution\n",
        "session = Session.builder.create()\n",
        "print(f\"Connected to Snowflake: {session.get_current_account()}\")\n",
        "\n",
        "# Enable Hybrid Execution\n",
        "AutoSwitchBackend.enable()\n",
        "print(f\"Hybrid execution enabled: {AutoSwitchBackend.get()}\")\n",
        "\n",
        "# Test basic hybrid functionality\n",
        "test_df = pd.DataFrame({'test': [1, 2, 3]})\n",
        "assert test_df.get_backend() == 'Pandas', f\"Expected 'Pandas' backend for small DataFrame, got {test_df.get_backend()}\"\n",
        "print(\"✓ Basic hybrid execution verified\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Create Test Data for Complex Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Creating Test Datasets ===\n",
            "Creating large sales transactions table...\n",
            "✓ Large sales table created (15M rows)\n",
            "✓ Customer demographics table created\n",
            "✓ Product catalog table created\n",
            "✓ All test datasets created successfully\n"
          ]
        }
      ],
      "source": [
        "# Create comprehensive test datasets for complex operations\n",
        "print(\"=== Creating Test Datasets ===\")\n",
        "\n",
        "# Large sales transactions table (15M rows) - will be in Snowflake\n",
        "print(\"Creating large sales transactions table...\")\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE TABLE sales_transactions (\n",
        "    transaction_id STRING,\n",
        "    customer_id INT,\n",
        "    product_id STRING,\n",
        "    category STRING,\n",
        "    subcategory STRING,\n",
        "    transaction_date DATE,\n",
        "    transaction_timestamp TIMESTAMP,\n",
        "    quantity INT,\n",
        "    unit_price DECIMAL(10,2),\n",
        "    total_amount DECIMAL(12,2),\n",
        "    discount_percent DECIMAL(5,2),\n",
        "    payment_method STRING,\n",
        "    store_location STRING,\n",
        "    sales_rep_id INT,\n",
        "    promotion_code STRING,\n",
        "    customer_segment STRING\n",
        ");\n",
        "\"\"\").collect()\n",
        "\n",
        "# Populate with synthetic data\n",
        "session.sql(\"\"\"\n",
        "INSERT INTO sales_transactions \n",
        "SELECT \n",
        "    UUID_STRING() as transaction_id,\n",
        "    UNIFORM(1, 500000, RANDOM()) as customer_id,\n",
        "    'PROD-' || LPAD(UNIFORM(1, 25000, RANDOM()), 5, '0') as product_id,\n",
        "    CASE UNIFORM(1, 8, RANDOM())\n",
        "        WHEN 1 THEN 'Electronics'\n",
        "        WHEN 2 THEN 'Clothing'\n",
        "        WHEN 3 THEN 'Home & Garden'\n",
        "        WHEN 4 THEN 'Sports & Outdoors'\n",
        "        WHEN 5 THEN 'Books'\n",
        "        WHEN 6 THEN 'Health & Beauty'\n",
        "        WHEN 7 THEN 'Automotive'\n",
        "        ELSE 'Food & Beverage'\n",
        "    END as category,\n",
        "    CASE UNIFORM(1, 15, RANDOM())\n",
        "        WHEN 1 THEN 'Smartphones' WHEN 2 THEN 'Laptops' WHEN 3 THEN 'T-Shirts'\n",
        "        WHEN 4 THEN 'Jeans' WHEN 5 THEN 'Furniture' WHEN 6 THEN 'Garden Tools'\n",
        "        WHEN 7 THEN 'Running Shoes' WHEN 8 THEN 'Fiction' WHEN 9 THEN 'Board Games'\n",
        "        WHEN 10 THEN 'Skincare' WHEN 11 THEN 'Car Parts' WHEN 12 THEN 'Supplements'\n",
        "        ELSE 'Miscellaneous'\n",
        "    END as subcategory,\n",
        "    DATEADD(DAY, UNIFORM(-365, 0, RANDOM()), CURRENT_DATE()) as transaction_date,\n",
        "    DATEADD(MINUTE, UNIFORM(0, 1440, RANDOM()), \n",
        "            DATEADD(DAY, UNIFORM(-365, 0, RANDOM()), CURRENT_DATE())) as transaction_timestamp,\n",
        "    UNIFORM(1, 8, RANDOM()) as quantity,\n",
        "    ROUND(UNIFORM(9.99, 899.99, RANDOM()), 2) as unit_price,\n",
        "    ROUND(UNIFORM(9.99, 899.99, RANDOM()) * UNIFORM(1, 8, RANDOM()) * \n",
        "          (1 - UNIFORM(0, 30, RANDOM())/100), 2) as total_amount,\n",
        "    ROUND(UNIFORM(0, 30, RANDOM()), 2) as discount_percent,\n",
        "    CASE UNIFORM(1, 6, RANDOM())\n",
        "        WHEN 1 THEN 'Credit Card' WHEN 2 THEN 'Debit Card' WHEN 3 THEN 'PayPal'\n",
        "        WHEN 4 THEN 'Cash' WHEN 5 THEN 'Bank Transfer' ELSE 'Buy Now Pay Later'\n",
        "    END as payment_method,\n",
        "    CASE UNIFORM(1, 12, RANDOM())\n",
        "        WHEN 1 THEN 'New York, NY' WHEN 2 THEN 'Los Angeles, CA' WHEN 3 THEN 'Chicago, IL'\n",
        "        WHEN 4 THEN 'Houston, TX' WHEN 5 THEN 'Phoenix, AZ' WHEN 6 THEN 'Philadelphia, PA'\n",
        "        WHEN 7 THEN 'San Antonio, TX' WHEN 8 THEN 'San Diego, CA' WHEN 9 THEN 'Dallas, TX'\n",
        "        WHEN 10 THEN 'San Jose, CA' WHEN 11 THEN 'Austin, TX' ELSE 'Seattle, WA'\n",
        "    END as store_location,\n",
        "    UNIFORM(1, 200, RANDOM()) as sales_rep_id,\n",
        "    CASE UNIFORM(1, 5, RANDOM())\n",
        "        WHEN 1 THEN 'SUMMER20' WHEN 2 THEN 'WINTER25' WHEN 3 THEN 'SPRING15'\n",
        "        WHEN 4 THEN 'FLASH10' ELSE NULL\n",
        "    END as promotion_code,\n",
        "    CASE UNIFORM(1, 4, RANDOM())\n",
        "        WHEN 1 THEN 'Premium' WHEN 2 THEN 'Standard' WHEN 3 THEN 'Basic' ELSE 'VIP'\n",
        "    END as customer_segment\n",
        "FROM TABLE(GENERATOR(ROWCOUNT => 100000));\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"✓ Large sales table created (15M rows)\")\n",
        "\n",
        "# Create customer demographics table\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE TABLE customer_demographics (\n",
        "    customer_id INT,\n",
        "    first_name STRING,\n",
        "    last_name STRING,\n",
        "    email STRING,\n",
        "    age INT,\n",
        "    city STRING,\n",
        "    state STRING,\n",
        "    country STRING,\n",
        "    customer_since DATE,\n",
        "    loyalty_tier STRING,\n",
        "    annual_income INT,\n",
        "    preferred_category STRING\n",
        ");\n",
        "\"\"\").collect()\n",
        "\n",
        "session.sql(\"\"\"\n",
        "INSERT INTO customer_demographics\n",
        "SELECT DISTINCT\n",
        "    customer_id,\n",
        "    CASE UNIFORM(1, 12, RANDOM())\n",
        "        WHEN 1 THEN 'John' WHEN 2 THEN 'Jane' WHEN 3 THEN 'Michael' WHEN 4 THEN 'Sarah'\n",
        "        WHEN 5 THEN 'David' WHEN 6 THEN 'Emily' WHEN 7 THEN 'Robert' WHEN 8 THEN 'Lisa'\n",
        "        WHEN 9 THEN 'William' WHEN 10 THEN 'Jennifer' WHEN 11 THEN 'James' ELSE 'Mary'\n",
        "    END as first_name,\n",
        "    CASE UNIFORM(1, 12, RANDOM())\n",
        "        WHEN 1 THEN 'Smith' WHEN 2 THEN 'Johnson' WHEN 3 THEN 'Williams' WHEN 4 THEN 'Brown'\n",
        "        WHEN 5 THEN 'Jones' WHEN 6 THEN 'Garcia' WHEN 7 THEN 'Miller' WHEN 8 THEN 'Davis'\n",
        "        WHEN 9 THEN 'Rodriguez' WHEN 10 THEN 'Martinez' WHEN 11 THEN 'Lopez' ELSE 'Wilson'\n",
        "    END as last_name,\n",
        "    'customer' || customer_id || '@email.com' as email,\n",
        "    UNIFORM(18, 75, RANDOM()) as age,\n",
        "    CASE UNIFORM(1, 8, RANDOM())\n",
        "        WHEN 1 THEN 'New York' WHEN 2 THEN 'Los Angeles' WHEN 3 THEN 'Chicago'\n",
        "        WHEN 4 THEN 'Houston' WHEN 5 THEN 'Phoenix' WHEN 6 THEN 'Philadelphia'\n",
        "        WHEN 7 THEN 'San Antonio' ELSE 'San Diego'\n",
        "    END as city,\n",
        "    'CA' as state,\n",
        "    'USA' as country,\n",
        "    DATEADD(DAY, UNIFORM(-1095, -30, RANDOM()), CURRENT_DATE()) as customer_since,\n",
        "    CASE UNIFORM(1, 4, RANDOM())\n",
        "        WHEN 1 THEN 'Bronze' WHEN 2 THEN 'Silver' WHEN 3 THEN 'Gold' ELSE 'Platinum'\n",
        "    END as loyalty_tier,\n",
        "    UNIFORM(25000, 150000, RANDOM()) as annual_income,\n",
        "    CASE UNIFORM(1, 8, RANDOM())\n",
        "        WHEN 1 THEN 'Electronics' WHEN 2 THEN 'Clothing' WHEN 3 THEN 'Home & Garden'\n",
        "        WHEN 4 THEN 'Sports & Outdoors' WHEN 5 THEN 'Books' WHEN 6 THEN 'Health & Beauty'\n",
        "        WHEN 7 THEN 'Automotive' ELSE 'Food & Beverage'\n",
        "    END as preferred_category\n",
        "FROM (SELECT DISTINCT customer_id FROM sales_transactions);\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"✓ Customer demographics table created\")\n",
        "\n",
        "# Create product catalog table\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE TABLE product_catalog (\n",
        "    product_id STRING,\n",
        "    product_name STRING,\n",
        "    category STRING,\n",
        "    subcategory STRING,\n",
        "    brand STRING,\n",
        "    cost_price DECIMAL(10,2),\n",
        "    retail_price DECIMAL(10,2),\n",
        "    weight_kg DECIMAL(8,3),\n",
        "    supplier_id INT,\n",
        "    launch_date DATE,\n",
        "    discontinued BOOLEAN\n",
        ");\n",
        "\"\"\").collect()\n",
        "\n",
        "session.sql(\"\"\"\n",
        "INSERT INTO product_catalog\n",
        "SELECT DISTINCT\n",
        "    product_id,\n",
        "    CASE UNIFORM(1, 8, RANDOM())\n",
        "        WHEN 1 THEN 'Premium ' || subcategory\n",
        "        WHEN 2 THEN 'Deluxe ' || subcategory\n",
        "        WHEN 3 THEN 'Standard ' || subcategory\n",
        "        WHEN 4 THEN 'Economy ' || subcategory\n",
        "        WHEN 5 THEN 'Pro ' || subcategory\n",
        "        WHEN 6 THEN 'Ultra ' || subcategory\n",
        "        WHEN 7 THEN 'Classic ' || subcategory\n",
        "        ELSE 'Basic ' || subcategory\n",
        "    END as product_name,\n",
        "    category,\n",
        "    subcategory,\n",
        "    CASE UNIFORM(1, 10, RANDOM())\n",
        "        WHEN 1 THEN 'BrandA' WHEN 2 THEN 'BrandB' WHEN 3 THEN 'BrandC'\n",
        "        WHEN 4 THEN 'BrandD' WHEN 5 THEN 'BrandE' WHEN 6 THEN 'BrandF'\n",
        "        WHEN 7 THEN 'BrandG' WHEN 8 THEN 'BrandH' WHEN 9 THEN 'BrandI'\n",
        "        ELSE 'BrandJ'\n",
        "    END as brand,\n",
        "    ROUND(UNIFORM(5.00, 400.00, RANDOM()), 2) as cost_price,\n",
        "    ROUND(UNIFORM(9.99, 899.99, RANDOM()), 2) as retail_price,\n",
        "    ROUND(UNIFORM(0.1, 25.0, RANDOM()), 3) as weight_kg,\n",
        "    UNIFORM(1, 50, RANDOM()) as supplier_id,\n",
        "    DATEADD(DAY, UNIFORM(-1825, -30, RANDOM()), CURRENT_DATE()) as launch_date,\n",
        "    CASE WHEN UNIFORM(1, 100, RANDOM()) <= 5 THEN TRUE ELSE FALSE END as discontinued\n",
        "FROM (SELECT DISTINCT product_id, category, subcategory FROM sales_transactions);\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"✓ Product catalog table created\")\n",
        "print(\"✓ All test datasets created successfully\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Load Data and Test Basic Backend Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Loading Datasets and Testing Backend Selection ===\n",
            "Sales data: 15,000,000 rows, backend: Snowflake\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "472274d11b974abf9a1198200f3e3669",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Transferring data from Snowflake to Pandas for 'modin.pandas.read_snowflake' with max estimated shape 500000x1…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Customer data: 500,000 rows, backend: Pandas\n",
            "Product data: 2,583,751 rows, backend: Snowflake\n",
            "Discount rules: 4 rows, backend: Pandas\n",
            "Category weights: 8 rows, backend: Pandas\n",
            "✓ All datasets loaded with correct backend selection\n"
          ]
        }
      ],
      "source": [
        "# Load datasets and verify backend selection\n",
        "print(\"=== Loading Datasets and Testing Backend Selection ===\")\n",
        "\n",
        "# Load large dataset - should use Snowflake backend\n",
        "sales_df = pd.read_snowflake(\"sales_transactions\")\n",
        "print(f\"Sales data: {len(sales_df):,} rows, backend: {sales_df.get_backend()}\")\n",
        "assert sales_df.get_backend() == 'Snowflake', f\"Expected Snowflake backend for large dataset\"\n",
        "\n",
        "# Load medium dataset - customer demographics\n",
        "customers_df = pd.read_snowflake(\"customer_demographics\") \n",
        "print(f\"Customer data: {len(customers_df):,} rows, backend: {customers_df.get_backend()}\")\n",
        "\n",
        "# Load product catalog\n",
        "products_df = pd.read_snowflake(\"product_catalog\")\n",
        "print(f\"Product data: {len(products_df):,} rows, backend: {products_df.get_backend()}\")\n",
        "\n",
        "# Create some small DataFrames for mixed operations - should use Pandas\n",
        "discount_rules = pd.DataFrame({\n",
        "    'LOYALTY_TIER': ['Bronze', 'Silver', 'Gold', 'Platinum'],\n",
        "    'discount_multiplier': [1.0, 1.05, 1.10, 1.15],\n",
        "    'free_shipping_threshold': [100, 75, 50, 25]\n",
        "})\n",
        "print(f\"Discount rules: {discount_rules.shape[0]} rows, backend: {discount_rules.get_backend()}\")\n",
        "assert discount_rules.get_backend() == 'Pandas', f\"Expected Pandas backend for small dataset\"\n",
        "\n",
        "category_weights = pd.DataFrame({\n",
        "    'CATEGORY': ['Electronics', 'Clothing', 'Home & Garden', 'Sports & Outdoors', \n",
        "                'Books', 'Health & Beauty', 'Automotive', 'Food & Beverage'],\n",
        "    'avg_weight_multiplier': [2.5, 0.3, 4.2, 1.8, 0.4, 0.2, 8.5, 1.2],\n",
        "    'shipping_cost_base': [15.99, 8.99, 19.99, 12.99, 5.99, 7.99, 25.99, 10.99]\n",
        "})\n",
        "print(f\"Category weights: {category_weights.shape[0]} rows, backend: {category_weights.get_backend()}\")\n",
        "assert category_weights.get_backend() == 'Pandas', f\"Expected Pandas backend for small dataset\"\n",
        "\n",
        "print(\"✓ All datasets loaded with correct backend selection\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Complex Transformations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Test 1: Window Functions and Rolling Operations ===\n",
            "\n",
            "1.1 Rolling revenue calculations...\n",
            "Daily sales aggregation backend: Snowflake\n"
          ]
        },
        {
          "ename": "SnowparkSQLException",
          "evalue": "(1304): 01bdde5e-0e10-eb82-000c-a90b016f953b: 002028 (42601): SQL compilation error:\nambiguous column name 'TOTAL_AMOUNT_AVERAGE'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mSnowparkSQLException\u001b[39m                      Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Calculate rank and percentiles\u001b[39;00m\n\u001b[32m     20\u001b[39m daily_sales[\u001b[33m'\u001b[39m\u001b[33mrevenue_rank\u001b[39m\u001b[33m'\u001b[39m] = daily_sales[\u001b[33m'\u001b[39m\u001b[33mTOTAL_AMOUNT\u001b[39m\u001b[33m'\u001b[39m].rank(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m daily_sales[\u001b[33m'\u001b[39m\u001b[33mrevenue_pct_rank\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdaily_sales\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTOTAL_AMOUNT\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpct\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRolling calculations backend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdaily_sales.get_backend()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDaily sales with rolling metrics shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdaily_sales.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/modin/core/storage_formats/pandas/query_compiler_caster.py:1144\u001b[39m, in \u001b[36mwrap_function_in_argument_caster.<locals>.f_with_argument_casting\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# We have to set the global Backend correctly for I/O methods like\u001b[39;00m\n\u001b[32m   1142\u001b[39m \u001b[38;5;66;03m# read_json() to use the correct backend.\u001b[39;00m\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(Backend=result_backend):\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m     result = \u001b[43mf_to_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[32m   1146\u001b[39m     original_castable,\n\u001b[32m   1147\u001b[39m     original_qc,\n\u001b[32m   1148\u001b[39m     new_castable,\n\u001b[32m   1149\u001b[39m ) \u001b[38;5;129;01min\u001b[39;00m inplace_update_trackers:\n\u001b[32m   1150\u001b[39m     new_qc = new_castable._get_query_compiler()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/modin/plugin/_internal/telemetry.py:480\u001b[39m, in \u001b[36msnowpark_pandas_telemetry_method_decorator.<locals>.wrap\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap\u001b[39m(*args, **kwargs):  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# add a `type: ignore` for this function definition because the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# hints in-line here. We'll fix up the type with a `cast` before\u001b[39;00m\n\u001b[32m    479\u001b[39m     \u001b[38;5;66;03m# returning the function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_telemetry_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_standalone_function\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproperty_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproperty_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproperty_method_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproperty_method_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/modin/plugin/_internal/telemetry.py:403\u001b[39m, in \u001b[36m_telemetry_helper\u001b[39m\u001b[34m(func, args, kwargs, is_standalone_function, property_name, property_method_type)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# Send Telemetry and Raise Error\u001b[39;00m\n\u001b[32m    391\u001b[39m     _send_snowpark_pandas_telemetry_helper(\n\u001b[32m    392\u001b[39m         session=session,\n\u001b[32m    393\u001b[39m         telemetry_type=error_to_telemetry_type(e),\n\u001b[32m   (...)\u001b[39m\u001b[32m    401\u001b[39m         method_call_count=method_call_count,\n\u001b[32m    402\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    405\u001b[39m \u001b[38;5;66;03m# Not inplace lazy APIs: add curr_api_call to the result\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;66;03m# In hybrid execution modin, the result may be a NativeQueryCompiler, so we need to check for snowpark_pandas_api_calls.\u001b[39;00m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_snowpark_pandas_dataframe_or_series_type(result) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[32m    408\u001b[39m     result._query_compiler, \u001b[33m\"\u001b[39m\u001b[33msnowpark_pandas_api_calls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    409\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/modin/plugin/_internal/telemetry.py:388\u001b[39m, in \u001b[36m_telemetry_helper\u001b[39m\u001b[34m(func, args, kwargs, is_standalone_function, property_name, property_method_type)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# query_history is a QueryHistory instance which is a Context Managers\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;66;03m# See example in https://github.com/snowflakedb/snowpark-python/blob/main/src/snowflake/snowpark/session.py#L2052\u001b[39;00m\n\u001b[32m    385\u001b[39m     \u001b[38;5;66;03m# Use `nullcontext` to handle `session` lacking `query_history` attribute without raising an exception.\u001b[39;00m\n\u001b[32m    386\u001b[39m     \u001b[38;5;66;03m# This prevents telemetry from interfering with regular API calls.\u001b[39;00m\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(session, \u001b[33m\"\u001b[39m\u001b[33mquery_history\u001b[39m\u001b[33m\"\u001b[39m, nullcontext)() \u001b[38;5;28;01mas\u001b[39;00m query_history:\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m         result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# Send Telemetry and Raise Error\u001b[39;00m\n\u001b[32m    391\u001b[39m     _send_snowpark_pandas_telemetry_helper(\n\u001b[32m    392\u001b[39m         session=session,\n\u001b[32m    393\u001b[39m         telemetry_type=error_to_telemetry_type(e),\n\u001b[32m   (...)\u001b[39m\u001b[32m    401\u001b[39m         method_call_count=method_call_count,\n\u001b[32m    402\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/modin/core/storage_formats/pandas/query_compiler_caster.py:1144\u001b[39m, in \u001b[36mwrap_function_in_argument_caster.<locals>.f_with_argument_casting\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# We have to set the global Backend correctly for I/O methods like\u001b[39;00m\n\u001b[32m   1142\u001b[39m \u001b[38;5;66;03m# read_json() to use the correct backend.\u001b[39;00m\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(Backend=result_backend):\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m     result = \u001b[43mf_to_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[32m   1146\u001b[39m     original_castable,\n\u001b[32m   1147\u001b[39m     original_qc,\n\u001b[32m   1148\u001b[39m     new_castable,\n\u001b[32m   1149\u001b[39m ) \u001b[38;5;129;01min\u001b[39;00m inplace_update_trackers:\n\u001b[32m   1150\u001b[39m     new_qc = new_castable._get_query_compiler()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/modin/logging/logger_decorator.py:149\u001b[39m, in \u001b[36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m start_time = perf_counter()\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m LogMode.get() == \u001b[33m\"\u001b[39m\u001b[33mdisable\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     result = \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     emit_metric(metric_name, perf_counter() - start_time)\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/modin/pandas/base.py:2483\u001b[39m, in \u001b[36mBasePandasDataset.rank\u001b[39m\u001b[34m(self, axis, method, numeric_only, na_option, ascending, pct)\u001b[39m\n\u001b[32m   2479\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2480\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo axis named None for object type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2481\u001b[39m     )\n\u001b[32m   2482\u001b[39m axis = \u001b[38;5;28mself\u001b[39m._get_axis_number(axis)\n\u001b[32m-> \u001b[39m\u001b[32m2483\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__constructor__\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_compiler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query_compiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2485\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_option\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_option\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mascending\u001b[49m\u001b[43m=\u001b[49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2491\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2492\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/modin/core/storage_formats/pandas/query_compiler_caster.py:1087\u001b[39m, in \u001b[36mwrap_function_in_argument_caster.<locals>.f_with_argument_casting\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1083\u001b[39m args_dict = MappingProxyType(bound_arguments.arguments)\n\u001b[32m   1085\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_query_compilers) < \u001b[32m2\u001b[39m:\n\u001b[32m   1086\u001b[39m     \u001b[38;5;66;03m# No need to check should_pin_result() again, since we have already done so above.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1087\u001b[39m     result_backend, cast_to_qc = \u001b[43m_maybe_switch_backend_pre_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_qc\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_qc_for_pre_op_switch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_of_wrapped_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_of_wrapped_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m        \u001b[49m\u001b[43marguments\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1094\u001b[39m     calculator: BackendCostCalculator = BackendCostCalculator(\n\u001b[32m   1095\u001b[39m         operation_arguments=args_dict,\n\u001b[32m   1096\u001b[39m         api_cls_name=class_of_wrapped_fn,\n\u001b[32m   1097\u001b[39m         operation=name,\n\u001b[32m   1098\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/modin/core/storage_formats/pandas/query_compiler_caster.py:632\u001b[39m, in \u001b[36m_maybe_switch_backend_pre_op\u001b[39m\u001b[34m(function_name, input_qc, class_of_wrapped_fn, arguments)\u001b[39m\n\u001b[32m    623\u001b[39m input_backend = input_qc.get_backend()\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    625\u001b[39m     function_name\n\u001b[32m    626\u001b[39m     \u001b[38;5;129;01min\u001b[39;00m _CLASS_AND_BACKEND_TO_PRE_OP_SWITCH_METHODS[\n\u001b[32m   (...)\u001b[39m\u001b[32m    630\u001b[39m     ]\n\u001b[32m    631\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m     result_backend = \u001b[43m_get_backend_for_auto_switch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_qc\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_qc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_of_wrapped_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_of_wrapped_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43marguments\u001b[49m\u001b[43m=\u001b[49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    639\u001b[39m     result_backend = input_backend\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/modin/core/storage_formats/pandas/query_compiler_caster.py:782\u001b[39m, in \u001b[36m_get_backend_for_auto_switch\u001b[39m\u001b[34m(input_qc, class_of_wrapped_fn, function_name, arguments)\u001b[39m\n\u001b[32m    775\u001b[39m best_backend = starting_backend\n\u001b[32m    777\u001b[39m stay_cost = input_qc.stay_cost(\n\u001b[32m    778\u001b[39m     api_cls_name=class_of_wrapped_fn,\n\u001b[32m    779\u001b[39m     operation=function_name,\n\u001b[32m    780\u001b[39m     arguments=arguments,\n\u001b[32m    781\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m data_max_shape = \u001b[43minput_qc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_max_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    783\u001b[39m emit_metric(\n\u001b[32m    784\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhybrid.auto.api.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_of_wrapped_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.group.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_group\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    785\u001b[39m     \u001b[32m1\u001b[39m,\n\u001b[32m    786\u001b[39m )\n\u001b[32m    787\u001b[39m emit_metric(\n\u001b[32m    788\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhybrid.auto.current.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstarting_backend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.group.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_group\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.stay_cost\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    789\u001b[39m     stay_cost,\n\u001b[32m    790\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/modin/logging/logger_decorator.py:149\u001b[39m, in \u001b[36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m start_time = perf_counter()\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m LogMode.get() == \u001b[33m\"\u001b[39m\u001b[33mdisable\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     result = \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     emit_metric(metric_name, perf_counter() - start_time)\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/modin/plugin/compiler/snowflake_query_compiler.py:796\u001b[39m, in \u001b[36mSnowflakeQueryCompiler._max_shape\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    791\u001b[39m \u001b[38;5;66;03m# hack to work around large numbers when things are an estimate\u001b[39;00m\n\u001b[32m    792\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    793\u001b[39m     ordered_dataframe.row_count_upper_bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    794\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m ordered_dataframe.row_count_upper_bound > \u001b[32m1e34\u001b[39m\n\u001b[32m    795\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m     num_rows = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_axis_len\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_rows \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    798\u001b[39m     num_rows = \u001b[32m10_000_000_000\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/modin/plugin/compiler/snowflake_query_compiler.py:508\u001b[39m, in \u001b[36m_propagate_attrs_on_methods.<locals>.propagate_attrs_decorator.<locals>.wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(method)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     result = \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    509\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, SnowflakeQueryCompiler) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._attrs):\n\u001b[32m    510\u001b[39m         result._attrs = copy.deepcopy(\u001b[38;5;28mself\u001b[39m._attrs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/modin/logging/logger_decorator.py:149\u001b[39m, in \u001b[36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m start_time = perf_counter()\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m LogMode.get() == \u001b[33m\"\u001b[39m\u001b[33mdisable\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     result = \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     emit_metric(metric_name, perf_counter() - start_time)\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/modin/plugin/compiler/snowflake_query_compiler.py:10040\u001b[39m, in \u001b[36mSnowflakeQueryCompiler.get_axis_len\u001b[39m\u001b[34m(self, axis)\u001b[39m\n\u001b[32m  10023\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_axis_len\u001b[39m(\n\u001b[32m  10024\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10025\u001b[39m     axis: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m  10026\u001b[39m ) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m  10027\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the length of the specified axis.\u001b[39;00m\n\u001b[32m  10028\u001b[39m \n\u001b[32m  10029\u001b[39m \u001b[33;03m    If axis = 0, return number of rows.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m  10038\u001b[39m \u001b[33;03m    Length of the specified axis.\u001b[39;00m\n\u001b[32m  10039\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m10040\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_modin_frame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_rows\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.columns)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/functools.py:998\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m    996\u001b[39m val = cache.get(\u001b[38;5;28mself\u001b[39m.attrname, _NOT_FOUND)\n\u001b[32m    997\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[32m--> \u001b[39m\u001b[32m998\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    999\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1000\u001b[39m         cache[\u001b[38;5;28mself\u001b[39m.attrname] = val\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/modin/plugin/_internal/frame.py:741\u001b[39m, in \u001b[36mInternalFrame.num_rows\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;129m@functools\u001b[39m.cached_property\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnum_rows\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    737\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    738\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    739\u001b[39m \u001b[33;03m        Number of rows in this frame.\u001b[39;00m\n\u001b[32m    740\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcount_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mordered_dataframe\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/modin/plugin/_internal/utils.py:1902\u001b[39m, in \u001b[36mcount_rows\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df.row_count\n\u001b[32m   1901\u001b[39m df = df.ensure_row_count_column()\n\u001b[32m-> \u001b[39m\u001b[32m1902\u001b[39m rowset = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrow_count_snowflake_quoted_identifier\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1903\u001b[39m row_count = \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rowset) == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m rowset[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m   1904\u001b[39m df.row_count = row_count\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/modin/plugin/_internal/ordered_dataframe.py:1941\u001b[39m, in \u001b[36mOrderedDataFrame.collect\u001b[39m\u001b[34m(self, statement_params, block, log_on_exception, case_sensitive)\u001b[39m\n\u001b[32m   1939\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1940\u001b[39m     statement_params.update(get_default_snowpark_pandas_statement_params())\n\u001b[32m-> \u001b[39m\u001b[32m1941\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msnowpark_dataframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/_internal/telemetry.py:289\u001b[39m, in \u001b[36mdf_collect_api_telemetry.<locals>.wrap\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ResourceUsageCollector() \u001b[38;5;28;01mas\u001b[39;00m resource_usage_collector:\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m         result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m     resource_usage = resource_usage_collector.get_resource_usage()\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/_internal/utils.py:1120\u001b[39m, in \u001b[36mpublicapi.<locals>.call_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1116\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33m_emit_ast\u001b[39m\u001b[33m\"\u001b[39m] = is_ast_enabled()\n\u001b[32m   1118\u001b[39m \u001b[38;5;66;03m# TODO: Could modify internal docstring to display that users should not modify the _emit_ast parameter.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/dataframe.py:755\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self, statement_params, block, log_on_exception, case_sensitive, _emit_ast)\u001b[39m\n\u001b[32m    752\u001b[39m     _, kwargs[DATAFRAME_AST_PARAMETER] = \u001b[38;5;28mself\u001b[39m._session._ast_batch.flush(stmt)\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m open_telemetry_context_manager(\u001b[38;5;28mself\u001b[39m.collect, \u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_internal_collect_with_tag_no_telemetry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/dataframe.py:825\u001b[39m, in \u001b[36mDataFrame._internal_collect_with_tag_no_telemetry\u001b[39m\u001b[34m(self, statement_params, block, data_type, log_on_exception, case_sensitive, **kwargs)\u001b[39m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_internal_collect_with_tag_no_telemetry\u001b[39m(\n\u001b[32m    813\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    814\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     \u001b[38;5;66;03m# we should always call this method instead of collect(), to make sure the\u001b[39;00m\n\u001b[32m    824\u001b[39m     \u001b[38;5;66;03m# query tag is set properly.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_conn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_statement_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_or_update_statement_params_with_query_tag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_statement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_tag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m            \u001b[49m\u001b[43mSKIP_LEVELS_THREE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcollect_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcollect_stacktrace_in_query_tag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    835\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:628\u001b[39m, in \u001b[36mServerConnection.execute\u001b[39m\u001b[34m(self, plan, to_pandas, to_iter, to_arrow, block, data_type, log_on_exception, case_sensitive, **kwargs)\u001b[39m\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    619\u001b[39m     is_in_stored_procedure()\n\u001b[32m    620\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m     )\n\u001b[32m    624\u001b[39m ):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    625\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    626\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAsync query is not supported in stored procedure yet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    627\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m result_set, result_meta = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_result_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_arrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto_arrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[32m    640\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result_set\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:400\u001b[39m, in \u001b[36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    395\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m ne.with_traceback(tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    397\u001b[39m ne = SnowparkClientExceptionMessages.SQL_EXCEPTION_FROM_PROGRAMMING_ERROR(\n\u001b[32m    398\u001b[39m     e, debug_context=debug_context\n\u001b[32m    399\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ne.with_traceback(tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:174\u001b[39m, in \u001b[36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msnowflake\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msnowpark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    170\u001b[39m     _enable_dataframe_trace_on_error,\n\u001b[32m    171\u001b[39m )\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m snowflake.connector.errors.ProgrammingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msnowflake\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msnowpark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_internal\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manalyzer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mselect_statement\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    177\u001b[39m         Selectable,\n\u001b[32m    178\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:750\u001b[39m, in \u001b[36mServerConnection.get_result_set\u001b[39m\u001b[34m(self, plan, to_pandas, to_iter, block, data_type, log_on_exception, case_sensitive, ignore_results, to_arrow, **kwargs)\u001b[39m\n\u001b[32m    748\u001b[39m     kwargs[DATAFRAME_AST_PARAMETER] = dataframe_ast\n\u001b[32m    749\u001b[39m is_final_query = i == \u001b[38;5;28mlen\u001b[39m(main_queries) - \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfinal_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_final_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_last\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m    \u001b[49m\u001b[43masync_job_plan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m    \u001b[49m\u001b[43masync_post_actions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpost_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_arrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto_arrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_final_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    766\u001b[39m placeholders[query.query_id_place_holder] = (\n\u001b[32m    767\u001b[39m     result[\u001b[33m\"\u001b[39m\u001b[33msfqid\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last \u001b[38;5;28;01melse\u001b[39;00m result.query_id\n\u001b[32m    768\u001b[39m )\n\u001b[32m    769\u001b[39m result_meta = get_new_description(\u001b[38;5;28mself\u001b[39m._cursor)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:136\u001b[39m, in \u001b[36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages.SERVER_SESSION_EXPIRED(\n\u001b[32m    133\u001b[39m         ex.cause\n\u001b[32m    134\u001b[39m     )\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:130\u001b[39m, in \u001b[36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages.SERVER_SESSION_HAS_BEEN_CLOSED()\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ReauthenticationRequest \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages.SERVER_SESSION_EXPIRED(\n\u001b[32m    133\u001b[39m         ex.cause\n\u001b[32m    134\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:524\u001b[39m, in \u001b[36mServerConnection.run_query\u001b[39m\u001b[34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, params, num_statements, ignore_results, async_post_actions, to_arrow, **kwargs)\u001b[39m\n\u001b[32m    522\u001b[39m         query_id_log = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex.sfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ex, \u001b[33m\"\u001b[39m\u001b[33msfqid\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    523\u001b[39m         logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to execute query\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_id_log\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[32m    526\u001b[39m \u001b[38;5;66;03m# fetch_pandas_all/batches() only works for SELECT statements\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[38;5;66;03m# We call fetchall() if fetch_pandas_all/batches() fails,\u001b[39;00m\n\u001b[32m    528\u001b[39m \u001b[38;5;66;03m# because when the query plan has multiple queries, it will\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# have non-select statements, and it shouldn't fail if the user\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# calls to_pandas() to execute the query.\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:509\u001b[39m, in \u001b[36mServerConnection.run_query\u001b[39m\u001b[34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, params, num_statements, ignore_results, async_post_actions, to_arrow, **kwargs)\u001b[39m\n\u001b[32m    507\u001b[39m     cached_analyze_attributes.clear_cache()\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m--> \u001b[39m\u001b[32m509\u001b[39m     results_cursor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_and_notify_query_listener\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecute query [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_cursor.sfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:448\u001b[39m, in \u001b[36mServerConnection.execute_and_notify_query_listener\u001b[39m\u001b[34m(self, query, **kwargs)\u001b[39m\n\u001b[32m    444\u001b[39m     err_query = ex.query \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ex, Error) \u001b[38;5;28;01melse\u001b[39;00m query\n\u001b[32m    445\u001b[39m     \u001b[38;5;28mself\u001b[39m.notify_query_listeners(\n\u001b[32m    446\u001b[39m         QueryRecord(sfqid, err_query, \u001b[38;5;28;01mFalse\u001b[39;00m), is_error=\u001b[38;5;28;01mTrue\u001b[39;00m, **notify_kwargs\n\u001b[32m    447\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[32m    450\u001b[39m notify_kwargs[\u001b[33m\"\u001b[39m\u001b[33mrequestId\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(results_cursor._request_id)\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m.notify_query_listeners(\n\u001b[32m    452\u001b[39m     QueryRecord(results_cursor.sfqid, results_cursor.query), **notify_kwargs\n\u001b[32m    453\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:439\u001b[39m, in \u001b[36mServerConnection.execute_and_notify_query_listener\u001b[39m\u001b[34m(self, query, **kwargs)\u001b[39m\n\u001b[32m    436\u001b[39m     notify_kwargs[\u001b[33m\"\u001b[39m\u001b[33mdataframeAst\u001b[39m\u001b[33m\"\u001b[39m] = kwargs[DATAFRAME_AST_PARAMETER]\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     results_cursor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    441\u001b[39m     notify_kwargs[\u001b[33m\"\u001b[39m\u001b[33mrequestId\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/connector/cursor.py:1137\u001b[39m, in \u001b[36mSnowflakeCursor.execute\u001b[39m\u001b[34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements, _force_qmark_paramstyle, _dataframe_ast)\u001b[39m\n\u001b[32m   1133\u001b[39m     is_integrity_error = (\n\u001b[32m   1134\u001b[39m         code == \u001b[33m\"\u001b[39m\u001b[33m100072\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1135\u001b[39m     )  \u001b[38;5;66;03m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[32m   1136\u001b[39m     error_class = IntegrityError \u001b[38;5;28;01mif\u001b[39;00m is_integrity_error \u001b[38;5;28;01melse\u001b[39;00m ProgrammingError\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m     \u001b[43mError\u001b[49m\u001b[43m.\u001b[49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/connector/errors.py:279\u001b[39m, in \u001b[36mError.errorhandler_wrapper\u001b[39m\u001b[34m(connection, cursor, error_class, error_value)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merrorhandler_wrapper\u001b[39m(\n\u001b[32m    258\u001b[39m     connection: SnowflakeConnection | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    261\u001b[39m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    262\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    263\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[32m    264\u001b[39m \n\u001b[32m    265\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    276\u001b[39m \u001b[33;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     handed_over = \u001b[43mError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[32m    286\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Error.errorhandler_make_exception(\n\u001b[32m    287\u001b[39m             error_class,\n\u001b[32m    288\u001b[39m             error_value,\n\u001b[32m    289\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/connector/errors.py:334\u001b[39m, in \u001b[36mError.hand_to_other_handler\u001b[39m\u001b[34m(connection, cursor, error_class, error_value)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    333\u001b[39m     cursor.messages.append((error_class, error_value))\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hybrid-pandas-dogfood-1.34.0-python-3.12/lib/python3.12/site-packages/snowflake/connector/errors.py:210\u001b[39m, in \u001b[36mError.default_errorhandler\u001b[39m\u001b[34m(connection, cursor, error_class, error_value)\u001b[39m\n\u001b[32m    208\u001b[39m errno = error_value.get(\u001b[33m\"\u001b[39m\u001b[33merrno\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    209\u001b[39m done_format_msg = error_value.get(\u001b[33m\"\u001b[39m\u001b[33mdone_format_msg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[32m    211\u001b[39m     msg=error_value.get(\u001b[33m\"\u001b[39m\u001b[33mmsg\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    212\u001b[39m     errno=\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[32m    213\u001b[39m     sqlstate=error_value.get(\u001b[33m\"\u001b[39m\u001b[33msqlstate\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    214\u001b[39m     sfqid=error_value.get(\u001b[33m\"\u001b[39m\u001b[33msfqid\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    215\u001b[39m     query=error_value.get(\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    216\u001b[39m     done_format_msg=(\n\u001b[32m    217\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[32m    218\u001b[39m     ),\n\u001b[32m    219\u001b[39m     connection=connection,\n\u001b[32m    220\u001b[39m     cursor=cursor,\n\u001b[32m    221\u001b[39m )\n",
            "\u001b[31mSnowparkSQLException\u001b[39m: (1304): 01bdde5e-0e10-eb82-000c-a90b016f953b: 002028 (42601): SQL compilation error:\nambiguous column name 'TOTAL_AMOUNT_AVERAGE'"
          ]
        }
      ],
      "source": [
        "# Test 1: Window Functions and Rolling Operations\n",
        "print(\"=== Test 1: Window Functions and Rolling Operations ===\\n\")\n",
        "\n",
        "# Create time series with rolling calculations\n",
        "print(\"1.1 Rolling revenue calculations...\")\n",
        "daily_sales = sales_df.groupby('TRANSACTION_DATE').agg({\n",
        "    'TOTAL_AMOUNT': 'sum',\n",
        "    'QUANTITY': 'sum',\n",
        "    'TRANSACTION_ID': 'count'\n",
        "}).sort_index()\n",
        "\n",
        "print(f\"Daily sales aggregation backend: {daily_sales.get_backend()}\")\n",
        "\n",
        "# Add rolling metrics\n",
        "daily_sales['revenue_7day_avg'] = daily_sales['TOTAL_AMOUNT'].rolling(window=7, min_periods=1).mean()\n",
        "daily_sales['revenue_30day_avg'] = daily_sales['TOTAL_AMOUNT'].rolling(window=30, min_periods=1).mean()\n",
        "daily_sales['revenue_7day_std'] = daily_sales['TOTAL_AMOUNT'].rolling(window=7, min_periods=1).std()\n",
        "\n",
        "# Calculate rank and percentiles\n",
        "daily_sales['revenue_rank'] = daily_sales['TOTAL_AMOUNT'].rank(ascending=False)\n",
        "daily_sales['revenue_pct_rank'] = daily_sales['TOTAL_AMOUNT'].rank(pct=True)\n",
        "\n",
        "print(f\"Rolling calculations backend: {daily_sales.get_backend()}\")\n",
        "print(f\"Daily sales with rolling metrics shape: {daily_sales.shape}\")\n",
        "print(f\"Sample rolling data:\\\\n{daily_sales.head()}\")\n",
        "\n",
        "# Test window functions on individual transactions\n",
        "print(\"\\\\n1.2 Customer ranking with window functions...\")\n",
        "customer_metrics = sales_df.groupby('CUSTOMER_ID').agg({\n",
        "    'TOTAL_AMOUNT': ['sum', 'count', 'mean'],\n",
        "    'TRANSACTION_DATE': ['min', 'max']\n",
        "})\n",
        "customer_metrics.columns = ['total_spent', 'transaction_count', 'avg_transaction', 'first_purchase', 'last_purchase']\n",
        "\n",
        "# Add ranking within segments\n",
        "customer_metrics['spending_rank'] = customer_metrics['total_spent'].rank(ascending=False)\n",
        "customer_metrics['spending_percentile'] = customer_metrics['total_spent'].rank(pct=True)\n",
        "\n",
        "print(f\"Customer metrics backend: {customer_metrics.get_backend()}\")\n",
        "print(f\"Customer metrics shape: {customer_metrics.shape}\")\n",
        "\n",
        "print(\"✓ Window functions and rolling operations completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2: Pivoting and Reshaping Operations\n",
        "print(\"=== Test 2: Pivoting and Reshaping Operations ===\\n\")\n",
        "\n",
        "print(\"2.1 Creating pivot tables...\")\n",
        "# Create a sample for pivoting (smaller dataset for performance)\n",
        "pivot_sample = sales_df.head(100000)\n",
        "print(f\"Pivot sample backend: {pivot_sample.get_backend()}\")\n",
        "\n",
        "# Pivot table: categories vs payment methods\n",
        "category_payment_pivot = pivot_sample.pivot_table(\n",
        "    values='TOTAL_AMOUNT',\n",
        "    index='CATEGORY',\n",
        "    columns='PAYMENT_METHOD',\n",
        "    aggfunc=['sum', 'mean'],\n",
        "    fill_value=0\n",
        ")\n",
        "print(f\"Pivot table backend: {category_payment_pivot.get_backend()}\")\n",
        "print(f\"Pivot table shape: {category_payment_pivot.shape}\")\n",
        "print(f\"Pivot table sample:\\\\n{category_payment_pivot.head()}\")\n",
        "\n",
        "# Cross-tabulation\n",
        "print(\"\\\\n2.2 Cross-tabulation analysis...\")\n",
        "crosstab_sample = sales_df.head(50000)\n",
        "category_segment_crosstab = pd.crosstab(\n",
        "    crosstab_sample['CATEGORY'],\n",
        "    crosstab_sample['CUSTOMER_SEGMENT'],\n",
        "    values=crosstab_sample['TOTAL_AMOUNT'],\n",
        "    aggfunc='sum',\n",
        "    margins=True\n",
        ")\n",
        "print(f\"Crosstab backend: {category_segment_crosstab.get_backend()}\")\n",
        "print(f\"Crosstab shape: {category_segment_crosstab.shape}\")\n",
        "\n",
        "# Melting/unpivoting operations\n",
        "print(\"\\\\n2.3 Melting operations...\")\n",
        "melt_sample = sales_df.head(10000)[['TRANSACTION_ID', 'QUANTITY', 'UNIT_PRICE', 'TOTAL_AMOUNT', 'DISCOUNT_PERCENT']]\n",
        "melted_data = melt_sample.melt(\n",
        "    id_vars=['TRANSACTION_ID'],\n",
        "    value_vars=['QUANTITY', 'UNIT_PRICE', 'TOTAL_AMOUNT', 'DISCOUNT_PERCENT'],\n",
        "    var_name='metric_type',\n",
        "    value_name='metric_value'\n",
        ")\n",
        "print(f\"Melted data backend: {melted_data.get_backend()}\")\n",
        "print(f\"Melted data shape: {melted_data.shape}\")\n",
        "\n",
        "print(\"✓ Pivoting and reshaping operations completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3: Advanced Groupby Operations with Complex Aggregations\n",
        "print(\"=== Test 3: Advanced Groupby Operations ===\\n\")\n",
        "\n",
        "print(\"3.1 Multi-level grouping with custom aggregations...\")\n",
        "# Complex groupby with multiple aggregation functions\n",
        "complex_agg = sales_df.groupby(['CATEGORY', 'CUSTOMER_SEGMENT', 'STORE_LOCATION']).agg({\n",
        "    'TOTAL_AMOUNT': ['sum', 'mean', 'std', 'count'],\n",
        "    'QUANTITY': ['sum', 'mean'],\n",
        "    'DISCOUNT_PERCENT': ['mean', 'max'],\n",
        "    'UNIT_PRICE': ['min', 'max', 'median']\n",
        "})\n",
        "\n",
        "print(f\"Complex aggregation backend: {complex_agg.get_backend()}\")\n",
        "print(f\"Complex aggregation shape: {complex_agg.shape}\")\n",
        "\n",
        "# Custom aggregation functions\n",
        "print(\"\\\\n3.2 Custom aggregation functions...\")\n",
        "def coefficient_of_variation(x):\n",
        "    return x.std() / x.mean() if x.mean() != 0 else 0\n",
        "\n",
        "def revenue_concentration(x):\n",
        "    \"\"\"Calculate how concentrated revenue is (higher = more concentrated)\"\"\"\n",
        "    sorted_values = x.sort_values(ascending=False)\n",
        "    total = sorted_values.sum()\n",
        "    if total == 0:\n",
        "        return 0\n",
        "    # Top 20% of transactions\n",
        "    top_20_pct_count = max(1, len(sorted_values) // 5)\n",
        "    top_20_pct_revenue = sorted_values.head(top_20_pct_count).sum()\n",
        "    return top_20_pct_revenue / total\n",
        "\n",
        "# Apply custom functions - this should work on a sample for performance\n",
        "custom_agg_sample = sales_df.head(50000)\n",
        "custom_agg = custom_agg_sample.groupby('CATEGORY').agg({\n",
        "    'TOTAL_AMOUNT': [\n",
        "        'sum', 'mean', 'count',\n",
        "        ('cv', coefficient_of_variation),\n",
        "        ('concentration', revenue_concentration)\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(f\"Custom aggregation backend: {custom_agg.get_backend()}\")\n",
        "print(f\"Custom aggregation shape: {custom_agg.shape}\")\n",
        "print(f\"Custom aggregation sample:\\\\n{custom_agg.head()}\")\n",
        "\n",
        "print(\"✓ Advanced groupby operations completed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Cross-Backend Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 4: Cross-Backend Joins and Merges\n",
        "print(\"=== Test 4: Cross-Backend Joins and Merges ===\\\\n\")\n",
        "\n",
        "print(\"4.1 Snowflake-to-Snowflake joins...\")\n",
        "# Join large datasets (both in Snowflake)\n",
        "sales_customer_join = sales_df.merge(\n",
        "    customers_df,\n",
        "    on='CUSTOMER_ID',\n",
        "    how='inner'\n",
        ")\n",
        "print(f\"Sales-Customer join backend: {sales_customer_join.get_backend()}\")\n",
        "print(f\"Sales-Customer join shape: {sales_customer_join.shape}\")\n",
        "assert sales_customer_join.get_backend() in ['Snowflake', 'Pandas'], f\"Expected valid backend for large join\"\n",
        "\n",
        "print(\"\\\\n4.2 Mixed backend joins (Snowflake + Pandas)...\")\n",
        "# Explicitly force sales data to Pandas to demonstrate cross-backend join\n",
        "sales_sample = sales_df.head(10000).set_backend('Pandas')\n",
        "print(f\"Sales sample backend: {sales_sample.get_backend()}\")\n",
        "assert sales_sample.get_backend() == 'Pandas', f\"Expected Pandas backend after forced switch\"\n",
        "\n",
        "# Merge Pandas DataFrame with Pandas DataFrame\n",
        "sales_with_discount = sales_sample.merge(\n",
        "    discount_rules,\n",
        "    left_on='CUSTOMER_SEGMENT', \n",
        "    right_on='LOYALTY_TIER',\n",
        "    how='left'\n",
        ")\n",
        "print(f\"Sales with discount rules backend: {sales_with_discount.get_backend()}\")\n",
        "print(f\"Sales with discount rules shape: {sales_with_discount.shape}\")\n",
        "\n",
        "# Calculate enhanced discount\n",
        "sales_with_discount['enhanced_discount'] = (\n",
        "    sales_with_discount['DISCOUNT_PERCENT'] * sales_with_discount['discount_multiplier']\n",
        ")\n",
        "sales_with_discount['qualifies_free_shipping'] = (\n",
        "    sales_with_discount['TOTAL_AMOUNT'] >= sales_with_discount['free_shipping_threshold']\n",
        ")\n",
        "\n",
        "print(f\"Enhanced discount calculation backend: {sales_with_discount.get_backend()}\")\n",
        "\n",
        "print(\"\\\\n4.3 Three-way join across different backends...\")\n",
        "# Complex three-way join - first join stays in Snowflake (large datasets)\n",
        "sales_products = sales_df.head(5000).merge(products_df, on='PRODUCT_ID', how='inner')\n",
        "print(f\"Sales-Products join backend: {sales_products.get_backend()}\")\n",
        "assert sales_products.get_backend() == 'Snowflake', f\"Expected Snowflake backend for large dataset join\"\n",
        "\n",
        "# Second join with small Pandas DataFrame - will move data to appropriate backend\n",
        "sales_products_categories = sales_products.merge(\n",
        "    category_weights,\n",
        "    left_on='category_x',  # from sales\n",
        "    right_on='CATEGORY',\n",
        "    how='left'\n",
        ")\n",
        "print(f\"Three-way join backend: {sales_products_categories.get_backend()}\")\n",
        "print(f\"Three-way join shape: {sales_products_categories.shape}\")\n",
        "# Assert the join result uses appropriate backend\n",
        "assert sales_products_categories.get_backend() in ['Pandas', 'Snowflake'], f\"Expected valid backend for three-way join\"\n",
        "print(f\"✓ ASSERTION PASSED: Three-way join uses {sales_products_categories.get_backend()} backend\")\n",
        "\n",
        "# Calculate shipping costs\n",
        "sales_products_categories['estimated_shipping'] = (\n",
        "    sales_products_categories['weight_kg'] * \n",
        "    sales_products_categories['avg_weight_multiplier'] * \n",
        "    sales_products_categories['shipping_cost_base']\n",
        ")\n",
        "\n",
        "print(f\"Shipping calculation backend: {sales_products_categories.get_backend()}\")\n",
        "\n",
        "print(\"✓ Cross-backend joins and merges completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 5: Concatenation and Union Operations\n",
        "print(\"=== Test 5: Concatenation and Union Operations ===\\\\n\")\n",
        "\n",
        "print(\"5.1 Concatenating DataFrames from different backends...\")\n",
        "# Create subsets with different backends\n",
        "snowflake_subset1 = sales_df.query('category == \"Electronics\"').head(5000)\n",
        "snowflake_subset2 = sales_df.query('category == \"Clothing\"').head(5000)\n",
        "print(f\"Snowflake subset 1 backend: {snowflake_subset1.get_backend()}\")\n",
        "print(f\"Snowflake subset 2 backend: {snowflake_subset2.get_backend()}\")\n",
        "\n",
        "# Force one to Pandas to test cross-backend concat\n",
        "pandas_subset = snowflake_subset2.set_backend('Pandas')\n",
        "print(f\"Pandas subset backend: {pandas_subset.get_backend()}\")\n",
        "\n",
        "# Concatenate across backends\n",
        "mixed_concat = pd.concat([snowflake_subset1, pandas_subset], ignore_index=True)\n",
        "print(f\"Mixed concatenation backend: {mixed_concat.get_backend()}\")\n",
        "print(f\"Mixed concatenation shape: {mixed_concat.shape}\")\n",
        "\n",
        "print(\"\\\\n5.2 Vertical and horizontal concatenation...\")\n",
        "# Horizontal concatenation with different data\n",
        "customer_summary = customers_df.head(1000)[['CUSTOMER_ID', 'age', 'ANNUAL_INCOME']]\n",
        "loyalty_summary = customers_df.head(1000)[['CUSTOMER_ID', 'LOYALTY_TIER', 'PREFERRED_CATEGORY']]\n",
        "\n",
        "# Force different backends\n",
        "customer_summary_pandas = customer_summary.set_backend('Pandas')\n",
        "print(f\"Customer summary backend: {customer_summary_pandas.get_backend()}\")\n",
        "print(f\"Loyalty summary backend: {loyalty_summary.get_backend()}\")\n",
        "\n",
        "# Horizontal concat (join on index)\n",
        "customer_combined = pd.concat([customer_summary_pandas, loyalty_summary.set_index('CUSTOMER_ID')], axis=1)\n",
        "print(f\"Horizontal concatenation backend: {customer_combined.get_backend()}\")\n",
        "print(f\"Horizontal concatenation shape: {customer_combined.shape}\")\n",
        "\n",
        "print(\"\\\\n5.3 Append operations with different schemas...\")\n",
        "# Create compatible schemas for append\n",
        "electronics_sales = sales_df.query('category == \"Electronics\"').head(2000)\n",
        "clothing_sales = sales_df.query('category == \"Clothing\"').head(2000)\n",
        "\n",
        "# Add category-specific columns\n",
        "electronics_with_warranty = electronics_sales.copy()\n",
        "electronics_with_warranty['warranty_months'] = 24\n",
        "electronics_with_warranty['tech_support'] = True\n",
        "\n",
        "clothing_with_size = clothing_sales.copy()\n",
        "clothing_with_size['size_category'] = 'M'\n",
        "clothing_with_size['seasonal'] = True\n",
        "\n",
        "# Append with different schemas (will align columns)\n",
        "combined_products = pd.concat([\n",
        "    electronics_with_warranty, \n",
        "    clothing_with_size\n",
        "], ignore_index=True, sort=False)\n",
        "\n",
        "print(f\"Combined products backend: {combined_products.get_backend()}\")\n",
        "print(f\"Combined products shape: {combined_products.shape}\")\n",
        "print(f\"Combined products columns: {len(combined_products.columns)}\")\n",
        "\n",
        "print(\"✓ Concatenation and union operations completed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Advanced Analytics with Mixed Backends\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 6: Advanced Analytics Pipeline with Backend Switching\n",
        "print(\"=== Test 6: Advanced Analytics Pipeline ===\\\\n\")\n",
        "\n",
        "print(\"6.1 Customer Lifetime Value (CLV) calculation...\")\n",
        "# Step 1: Customer transaction summary (Snowflake)\n",
        "customer_transactions = sales_df.groupby('CUSTOMER_ID').agg({\n",
        "    'TOTAL_AMOUNT': ['sum', 'mean', 'count'],\n",
        "    'TRANSACTION_DATE': ['min', 'max'],\n",
        "    'DISCOUNT_PERCENT': 'mean'\n",
        "})\n",
        "customer_transactions.columns = ['total_spent', 'avg_order_value', 'order_count', 'first_purchase', 'last_purchase', 'avg_discount']\n",
        "\n",
        "print(f\"Customer transactions backend: {customer_transactions.get_backend()}\")\n",
        "print(f\"Customer transactions shape: {customer_transactions.shape}\")\n",
        "\n",
        "# Step 2: Add recency and frequency metrics\n",
        "today = pd.Timestamp.now().date()\n",
        "customer_transactions['days_since_last_purchase'] = (today - customer_transactions['last_purchase']).dt.days\n",
        "customer_transactions['customer_lifetime_days'] = (customer_transactions['last_purchase'] - customer_transactions['first_purchase']).dt.days\n",
        "customer_transactions['purchase_frequency'] = customer_transactions['order_count'] / (customer_transactions['customer_lifetime_days'] + 1) * 365\n",
        "\n",
        "# Step 3: Merge with demographics (cross-backend operation)\n",
        "customer_clv = customer_transactions.merge(customers_df, on='CUSTOMER_ID', how='inner')\n",
        "print(f\"Customer CLV merged backend: {customer_clv.get_backend()}\")\n",
        "\n",
        "# Step 4: Calculate CLV score using Pandas operations\n",
        "customer_clv_sample = customer_clv.head(10000).set_backend('Pandas')  # Force to Pandas for complex calculations\n",
        "print(f\"Customer CLV sample backend: {customer_clv_sample.get_backend()}\")\n",
        "\n",
        "# Complex CLV calculation\n",
        "customer_clv_sample['recency_score'] = pd.qcut(customer_clv_sample['days_since_last_purchase'], 5, labels=[5,4,3,2,1], duplicates='drop')\n",
        "customer_clv_sample['frequency_score'] = pd.qcut(customer_clv_sample['purchase_frequency'].rank(method='first'), 5, labels=[1,2,3,4,5], duplicates='drop')\n",
        "customer_clv_sample['monetary_score'] = pd.qcut(customer_clv_sample['total_spent'], 5, labels=[1,2,3,4,5], duplicates='drop')\n",
        "\n",
        "# Convert to numeric for calculation\n",
        "customer_clv_sample['recency_score'] = pd.to_numeric(customer_clv_sample['recency_score'])\n",
        "customer_clv_sample['frequency_score'] = pd.to_numeric(customer_clv_sample['frequency_score'])\n",
        "customer_clv_sample['monetary_score'] = pd.to_numeric(customer_clv_sample['monetary_score'])\n",
        "\n",
        "customer_clv_sample['clv_score'] = (\n",
        "    customer_clv_sample['recency_score'] * 0.3 +\n",
        "    customer_clv_sample['frequency_score'] * 0.3 +\n",
        "    customer_clv_sample['monetary_score'] * 0.4\n",
        ")\n",
        "\n",
        "print(f\"CLV calculation backend: {customer_clv_sample.get_backend()}\")\n",
        "print(f\"CLV sample:\\\\n{customer_clv_sample[['CUSTOMER_ID', 'total_spent', 'purchase_frequency', 'clv_score']].head()}\")\n",
        "\n",
        "print(\"\\\\n6.2 Product recommendation scoring...\")\n",
        "# Product affinity analysis\n",
        "product_customer_matrix = sales_df.head(20000).pivot_table(\n",
        "    values='TOTAL_AMOUNT',\n",
        "    index='CUSTOMER_ID',\n",
        "    columns='CATEGORY',\n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ")\n",
        "print(f\"Product-customer matrix backend: {product_customer_matrix.get_backend()}\")\n",
        "print(f\"Product-customer matrix shape: {product_customer_matrix.shape}\")\n",
        "\n",
        "# Calculate correlation matrix for product recommendations\n",
        "correlation_matrix = product_customer_matrix.corr()\n",
        "print(f\"Correlation matrix backend: {correlation_matrix.get_backend()}\")\n",
        "print(f\"Product correlations:\\\\n{correlation_matrix.head()}\")\n",
        "\n",
        "print(\"✓ Advanced analytics pipeline completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 7: Time Series Analysis with Complex Transformations\n",
        "print(\"=== Test 7: Time Series Analysis ===\\\\n\")\n",
        "\n",
        "print(\"7.1 Multi-frequency time series aggregation...\")\n",
        "# Daily, weekly, monthly aggregations\n",
        "daily_metrics = sales_df.groupby('TRANSACTION_DATE').agg({\n",
        "    'TOTAL_AMOUNT': ['sum', 'mean', 'count'],\n",
        "    'QUANTITY': 'sum',\n",
        "    'CUSTOMER_ID': 'nunique'\n",
        "})\n",
        "daily_metrics.columns = ['daily_revenue', 'avg_transaction', 'transaction_count', 'daily_quantity', 'unique_customers']\n",
        "print(f\"Daily metrics backend: {daily_metrics.get_backend()}\")\n",
        "\n",
        "# Add derived time features\n",
        "daily_metrics['day_of_week'] = daily_metrics.index.day_name()\n",
        "daily_metrics['month'] = daily_metrics.index.month\n",
        "daily_metrics['quarter'] = daily_metrics.index.quarter\n",
        "\n",
        "# Weekly aggregation\n",
        "weekly_metrics = daily_metrics.resample('W').agg({\n",
        "    'daily_revenue': 'sum',\n",
        "    'transaction_count': 'sum',\n",
        "    'unique_customers': 'sum'\n",
        "})\n",
        "weekly_metrics.columns = ['weekly_revenue', 'weekly_transactions', 'weekly_customers']\n",
        "print(f\"Weekly metrics backend: {weekly_metrics.get_backend()}\")\n",
        "\n",
        "# Monthly aggregation with growth rates\n",
        "monthly_metrics = daily_metrics.resample('M').agg({\n",
        "    'daily_revenue': 'sum',\n",
        "    'transaction_count': 'sum',\n",
        "    'unique_customers': 'mean'\n",
        "})\n",
        "monthly_metrics.columns = ['monthly_revenue', 'monthly_transactions', 'avg_daily_customers']\n",
        "\n",
        "# Calculate growth rates\n",
        "monthly_metrics['revenue_growth'] = monthly_metrics['monthly_revenue'].pct_change()\n",
        "monthly_metrics['transaction_growth'] = monthly_metrics['monthly_transactions'].pct_change()\n",
        "\n",
        "print(f\"Monthly metrics backend: {monthly_metrics.get_backend()}\")\n",
        "print(f\"Monthly growth metrics:\\\\n{monthly_metrics.tail()}\")\n",
        "\n",
        "print(\"\\\\n7.2 Seasonal decomposition and trend analysis...\")\n",
        "# Calculate moving averages for trend analysis\n",
        "daily_metrics['revenue_7day_ma'] = daily_metrics['daily_revenue'].rolling(window=7, center=True).mean()\n",
        "daily_metrics['revenue_30day_ma'] = daily_metrics['daily_revenue'].rolling(window=30, center=True).mean()\n",
        "\n",
        "# Seasonal patterns by day of week\n",
        "dow_patterns = daily_metrics.groupby('day_of_week').agg({\n",
        "    'daily_revenue': ['mean', 'std'],\n",
        "    'transaction_count': 'mean',\n",
        "    'unique_customers': 'mean'\n",
        "})\n",
        "\n",
        "print(f\"Day of week patterns backend: {dow_patterns.get_backend()}\")\n",
        "print(f\"Seasonal patterns:\\\\n{dow_patterns}\")\n",
        "\n",
        "print(\"\\\\n7.3 Cohort analysis...\")\n",
        "# Customer cohort analysis\n",
        "customer_first_purchase = sales_df.groupby('CUSTOMER_ID')['TRANSACTION_DATE'].min().reset_index()\n",
        "customer_first_purchase.columns = ['CUSTOMER_ID', 'cohort_month']\n",
        "customer_first_purchase['cohort_month'] = customer_first_purchase['cohort_month'].dt.to_period('M')\n",
        "\n",
        "print(f\"Customer cohorts backend: {customer_first_purchase.get_backend()}\")\n",
        "\n",
        "# Merge back with sales data for cohort analysis\n",
        "sales_with_cohort = sales_df.merge(customer_first_purchase, on='CUSTOMER_ID', how='left')\n",
        "sales_with_cohort['transaction_period'] = sales_with_cohort['TRANSACTION_DATE'].dt.to_period('M')\n",
        "\n",
        "print(f\"Sales with cohort backend: {sales_with_cohort.get_backend()}\")\n",
        "\n",
        "# Calculate period number for each customer\n",
        "sales_with_cohort['period_number'] = (\n",
        "    sales_with_cohort['transaction_period'] - sales_with_cohort['cohort_month']\n",
        ").apply(lambda x: x.n)\n",
        "\n",
        "# Cohort table\n",
        "cohort_data = sales_with_cohort.groupby(['cohort_month', 'period_number'])['CUSTOMER_ID'].nunique().unstack(level=1)\n",
        "cohort_data = cohort_data.divide(cohort_data.iloc[:, 0], axis=0)  # Retention rates\n",
        "\n",
        "print(f\"Cohort retention backend: {cohort_data.get_backend()}\")\n",
        "print(f\"Cohort retention rates:\\\\n{cohort_data.head()}\")\n",
        "\n",
        "print(\"✓ Time series analysis completed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Performance Analysis and Backend Switching Intelligence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 8: Performance Analysis and Backend Optimization\n",
        "print(\"=== Test 8: Performance Analysis ===\\\\n\")\n",
        "\n",
        "print(\"8.1 Comparing operations across backends...\")\n",
        "# Test same operation on different backends\n",
        "large_sample = sales_df.head(100000)\n",
        "print(f\"Large sample backend: {large_sample.get_backend()}\")\n",
        "# Assert large sample stays in Snowflake \n",
        "assert large_sample.get_backend() == 'Snowflake', f\"Expected Snowflake backend for 100K rows, got {large_sample.get_backend()}\"\n",
        "\n",
        "# Explicitly force to Pandas for comparison\n",
        "pandas_sample = large_sample.set_backend('Pandas')\n",
        "print(f\"Pandas sample backend: {pandas_sample.get_backend()}\")\n",
        "assert pandas_sample.get_backend() == 'Pandas', f\"Expected Pandas backend after forced switch\"\n",
        "\n",
        "# Complex aggregation on Snowflake\n",
        "start_time = time.time()\n",
        "snowflake_agg = large_sample.groupby(['CATEGORY', 'PAYMENT_METHOD']).agg({\n",
        "    'TOTAL_AMOUNT': ['sum', 'mean', 'std'],\n",
        "    'QUANTITY': ['sum', 'mean'],\n",
        "    'DISCOUNT_PERCENT': 'mean'\n",
        "})\n",
        "snowflake_time = time.time() - start_time\n",
        "print(f\"Snowflake aggregation: {snowflake_time:.3f} seconds, backend: {snowflake_agg.get_backend()}\")\n",
        "\n",
        "# Same aggregation on Pandas\n",
        "start_time = time.time()\n",
        "pandas_agg = pandas_sample.groupby(['CATEGORY', 'PAYMENT_METHOD']).agg({\n",
        "    'TOTAL_AMOUNT': ['sum', 'mean', 'std'],\n",
        "    'QUANTITY': ['sum', 'mean'],\n",
        "    'DISCOUNT_PERCENT': 'mean'\n",
        "})\n",
        "pandas_time = time.time() - start_time\n",
        "print(f\"Pandas aggregation: {pandas_time:.3f} seconds, backend: {pandas_agg.get_backend()}\")\n",
        "\n",
        "print(f\"Performance ratio (Pandas/Snowflake): {pandas_time/snowflake_time:.2f}x\")\n",
        "\n",
        "print(\"\\\\n8.2 Testing backend switching scenarios...\")\n",
        "# Scenario 1: Large to small pipeline\n",
        "print(\"Scenario 1: Large dataset -> filter -> small result\")\n",
        "large_data = sales_df\n",
        "filter_step = large_data[large_data['TOTAL_AMOUNT'] > 1000]\n",
        "small_result = filter_step.head(100)\n",
        "\n",
        "print(f\"  Large data: {large_data.get_backend()}\")\n",
        "print(f\"  After filter: {filter_step.get_backend()}\")\n",
        "print(f\"  Small result: {small_result.get_backend()}\")\n",
        "\n",
        "# Assert expected behavior in pipeline\n",
        "assert large_data.get_backend() == 'Snowflake', f\"Expected large data in Snowflake\"\n",
        "assert filter_step.get_backend() == 'Snowflake', f\"Expected filtered data in Snowflake (lazy evaluation)\"\n",
        "# Small result may switch to Pandas depending on hybrid execution logic\n",
        "assert small_result.get_backend() in ['Pandas', 'Snowflake'], f\"Expected valid backend for small result\"\n",
        "print(f\"  ✓ ASSERTIONS PASSED: Pipeline uses appropriate backends\")\n",
        "\n",
        "# Scenario 2: Cross-backend operations\n",
        "print(\"\\\\nScenario 2: Cross-backend merge\")\n",
        "snowflake_data = sales_df.head(5000)\n",
        "pandas_data = discount_rules.copy()\n",
        "\n",
        "print(f\"  Snowflake data: {snowflake_data.get_backend()}\")\n",
        "print(f\"  Pandas data: {pandas_data.get_backend()}\")\n",
        "\n",
        "# Assert initial backends are as expected\n",
        "assert snowflake_data.get_backend() in ['Snowflake', 'Pandas'], f\"Expected valid backend for 5K sample\"\n",
        "assert pandas_data.get_backend() == 'Pandas', f\"Expected Pandas backend for small lookup table\"\n",
        "\n",
        "merged_result = snowflake_data.merge(\n",
        "    pandas_data,\n",
        "    left_on='CUSTOMER_SEGMENT',\n",
        "    right_on='LOYALTY_TIER',\n",
        "    how='left'\n",
        ")\n",
        "print(f\"  Merged result: {merged_result.get_backend()}\")\n",
        "\n",
        "# Assert merge result uses appropriate backend\n",
        "assert merged_result.get_backend() in ['Pandas', 'Snowflake'], f\"Expected valid backend for merge result\"\n",
        "print(f\"  ✓ ASSERTIONS PASSED: Cross-backend merge uses {merged_result.get_backend()} backend\")\n",
        "\n",
        "# Scenario 3: Chained operations\n",
        "print(\"\\\\nScenario 3: Chained operations pipeline\")\n",
        "pipeline_result = (sales_df\n",
        "                  .query('category in [\"Electronics\", \"Clothing\"]')\n",
        "                  .groupby(['CATEGORY', 'STORE_LOCATION'])\n",
        "                  .agg({'TOTAL_AMOUNT': 'sum', 'QUANTITY': 'sum'})\n",
        "                  .sort_values('TOTAL_AMOUNT', ascending=False)\n",
        "                  .head(20))\n",
        "\n",
        "print(f\"  Pipeline result: {pipeline_result.get_backend()}\")\n",
        "print(f\"  Pipeline result shape: {pipeline_result.shape}\")\n",
        "\n",
        "# Assert chained operations result uses appropriate backend\n",
        "assert pipeline_result.get_backend() in ['Pandas', 'Snowflake'], f\"Expected valid backend for pipeline result\"\n",
        "# The final result has only 20 rows, so it may switch to Pandas for efficiency\n",
        "print(f\"  ✓ ASSERTION PASSED: Chained pipeline uses {pipeline_result.get_backend()} backend for {pipeline_result.shape[0]} rows\")\n",
        "\n",
        "print(\"\\\\n8.3 Backend switching monitoring...\")\n",
        "try:\n",
        "    pd.explain_switch()\n",
        "    print(\"✓ Backend switching explanations available\")\n",
        "except Exception as e:\n",
        "    print(f\"Backend switching explanations not available: {e}\")\n",
        "\n",
        "print(\"✓ Performance analysis completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 9: Complex Multi-Step Analytics Workflow\n",
        "print(\"=== Test 9: Complex Multi-Step Analytics Workflow ===\\\\n\")\n",
        "\n",
        "print(\"9.1 E-commerce Intelligence Pipeline...\")\n",
        "# Step 1: Customer segmentation based on behavior\n",
        "customer_behavior = sales_df.groupby('CUSTOMER_ID').agg({\n",
        "    'TOTAL_AMOUNT': ['sum', 'mean', 'count'],\n",
        "    'TRANSACTION_DATE': ['min', 'max'],\n",
        "    'CATEGORY': lambda x: x.nunique(),  # Category diversity\n",
        "    'STORE_LOCATION': lambda x: x.nunique(),  # Store diversity\n",
        "    'DISCOUNT_PERCENT': 'mean'\n",
        "})\n",
        "\n",
        "customer_behavior.columns = ['total_spent', 'avg_order_value', 'order_count', 'first_purchase', 'last_purchase', 'category_diversity', 'store_diversity', 'avg_discount_used']\n",
        "print(f\"Customer behavior analysis backend: {customer_behavior.get_backend()}\")\n",
        "\n",
        "# Step 2: Add customer demographics\n",
        "enriched_customers = customer_behavior.merge(customers_df, on='CUSTOMER_ID', how='inner')\n",
        "print(f\"Enriched customers backend: {enriched_customers.get_backend()}\")\n",
        "\n",
        "# Step 3: Calculate customer scores (force to Pandas for complex calculations)\n",
        "customer_scoring = enriched_customers.head(10000).set_backend('Pandas')\n",
        "print(f\"Customer scoring backend: {customer_scoring.get_backend()}\")\n",
        "\n",
        "# RFM scoring\n",
        "customer_scoring['recency_days'] = (pd.Timestamp.now().date() - customer_scoring['last_purchase']).dt.days\n",
        "customer_scoring['frequency_score'] = pd.qcut(customer_scoring['order_count'], 5, labels=[1,2,3,4,5], duplicates='drop')\n",
        "customer_scoring['monetary_score'] = pd.qcut(customer_scoring['total_spent'], 5, labels=[1,2,3,4,5], duplicates='drop')\n",
        "customer_scoring['recency_score'] = pd.qcut(customer_scoring['recency_days'], 5, labels=[5,4,3,2,1], duplicates='drop')\n",
        "\n",
        "# Convert to numeric\n",
        "customer_scoring['frequency_score'] = pd.to_numeric(customer_scoring['frequency_score'])\n",
        "customer_scoring['monetary_score'] = pd.to_numeric(customer_scoring['monetary_score'])\n",
        "customer_scoring['recency_score'] = pd.to_numeric(customer_scoring['recency_score'])\n",
        "\n",
        "# Customer value segments\n",
        "customer_scoring['customer_value'] = (\n",
        "    customer_scoring['recency_score'] * 0.3 +\n",
        "    customer_scoring['frequency_score'] * 0.3 +\n",
        "    customer_scoring['monetary_score'] * 0.4\n",
        ")\n",
        "\n",
        "def assign_segment(row):\n",
        "    if row['customer_value'] >= 4.0:\n",
        "        return 'Champions'\n",
        "    elif row['customer_value'] >= 3.5:\n",
        "        return 'Loyal Customers'\n",
        "    elif row['customer_value'] >= 3.0:\n",
        "        return 'Potential Loyalists'\n",
        "    elif row['customer_value'] >= 2.5:\n",
        "        return 'New Customers'\n",
        "    else:\n",
        "        return 'At Risk'\n",
        "\n",
        "customer_scoring['value_segment'] = customer_scoring.apply(assign_segment, axis=1)\n",
        "\n",
        "print(f\"Customer segmentation completed, backend: {customer_scoring.get_backend()}\")\n",
        "print(f\"Segment distribution:\\\\n{customer_scoring['value_segment'].value_counts()}\")\n",
        "\n",
        "print(\"\\\\n9.2 Product performance analysis...\")\n",
        "# Product performance with cross-backend operations\n",
        "product_sales = sales_df.groupby('PRODUCT_ID').agg({\n",
        "    'TOTAL_AMOUNT': ['sum', 'count'],\n",
        "    'QUANTITY': 'sum',\n",
        "    'CUSTOMER_ID': 'nunique'\n",
        "})\n",
        "product_sales.columns = ['total_revenue', 'transaction_count', 'total_quantity', 'unique_customers']\n",
        "\n",
        "# Merge with product catalog\n",
        "product_performance = product_sales.merge(products_df, on='PRODUCT_ID', how='inner')\n",
        "print(f\"Product performance backend: {product_performance.get_backend()}\")\n",
        "\n",
        "# Calculate profitability metrics\n",
        "product_performance['revenue_per_transaction'] = product_performance['total_revenue'] / product_performance['transaction_count']\n",
        "product_performance['profit_margin'] = (product_performance['retail_price'] - product_performance['cost_price']) / product_performance['retail_price']\n",
        "product_performance['estimated_profit'] = product_performance['total_quantity'] * (product_performance['retail_price'] - product_performance['cost_price'])\n",
        "\n",
        "# Category-level insights\n",
        "category_insights = product_performance.groupby('CATEGORY').agg({\n",
        "    'total_revenue': 'sum',\n",
        "    'estimated_profit': 'sum',\n",
        "    'unique_customers': 'sum',\n",
        "    'profit_margin': 'mean'\n",
        "})\n",
        "\n",
        "print(f\"Category insights backend: {category_insights.get_backend()}\")\n",
        "print(f\"Top categories by profit:\\\\n{category_insights.sort_values('estimated_profit', ascending=False).head()}\")\n",
        "\n",
        "print(\"\\\\n9.3 Market basket analysis...\")\n",
        "# Simple market basket analysis\n",
        "basket_data = sales_df.head(50000).groupby('CUSTOMER_ID')['CATEGORY'].apply(list).reset_index()\n",
        "basket_data['basket_size'] = basket_data['CATEGORY'].apply(len)\n",
        "basket_data['unique_categories'] = basket_data['CATEGORY'].apply(lambda x: len(set(x)))\n",
        "\n",
        "print(f\"Market basket data backend: {basket_data.get_backend()}\")\n",
        "print(f\"Average basket metrics:\")\n",
        "print(f\"  Basket size: {basket_data['basket_size'].mean():.2f}\")\n",
        "print(f\"  Category diversity: {basket_data['unique_categories'].mean():.2f}\")\n",
        "\n",
        "print(\"✓ Complex multi-step analytics workflow completed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Advanced Data Movement and Backend Control\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 10: Advanced Backend Control and Data Movement\n",
        "print(\"=== Test 10: Advanced Backend Control ===\\\\n\")\n",
        "\n",
        "print(\"10.1 Strategic backend pinning for complex workflows...\")\n",
        "# Pin large dataset to Snowflake for aggregation-heavy operations\n",
        "large_pinned = sales_df.pin_backend(inplace=False)\n",
        "print(f\"Large dataset pinned to: {large_pinned.get_backend()}\")\n",
        "\n",
        "# Perform operations on pinned data - assert it stays in Snowflake\n",
        "small_sample = large_pinned.head(1000)\n",
        "small_agg = small_sample.groupby('CATEGORY')['TOTAL_AMOUNT'].sum()\n",
        "\n",
        "print(f\"Small sample (pinned source): {small_sample.get_backend()}\")\n",
        "print(f\"Small aggregation (pinned source): {small_agg.get_backend()}\")\n",
        "\n",
        "# Assert pinned data stays in Snowflake even for small operations\n",
        "assert small_sample.get_backend() == 'Snowflake', f\"Expected pinned data to stay in Snowflake, got {small_sample.get_backend()}\"\n",
        "assert small_agg.get_backend() == 'Snowflake', f\"Expected pinned aggregation to stay in Snowflake, got {small_agg.get_backend()}\"\n",
        "print(\"✓ ASSERTION PASSED: Pinned data stays in Snowflake for small operations\")\n",
        "\n",
        "# Compare with unpinned behavior - may switch based on hybrid logic\n",
        "unpinned = large_pinned.unpin_backend()\n",
        "unpinned_sample = unpinned.head(1000)\n",
        "unpinned_agg = unpinned_sample.groupby('CATEGORY')['TOTAL_AMOUNT'].sum()\n",
        "\n",
        "print(f\"Small sample (unpinned): {unpinned_sample.get_backend()}\")\n",
        "print(f\"Small aggregation (unpinned): {unpinned_agg.get_backend()}\")\n",
        "\n",
        "# Assert unpinned behavior allows hybrid execution to choose optimal backend\n",
        "assert unpinned_sample.get_backend() in ['Pandas', 'Snowflake'], f\"Expected valid backend for unpinned sample\"\n",
        "assert unpinned_agg.get_backend() in ['Pandas', 'Snowflake'], f\"Expected valid backend for unpinned aggregation\"\n",
        "print(f\"✓ ASSERTION PASSED: Unpinned data uses {unpinned_sample.get_backend()} backend, aggregation uses {unpinned_agg.get_backend()}\")\n",
        "\n",
        "print(\"\\\\n10.2 Optimal backend selection for different operation types...\")\n",
        "# Test different operation types\n",
        "operations_test = sales_df.head(20000)\n",
        "\n",
        "# Aggregation-heavy (should prefer Snowflake)\n",
        "agg_result = operations_test.groupby(['CATEGORY', 'PAYMENT_METHOD', 'STORE_LOCATION']).agg({\n",
        "    'TOTAL_AMOUNT': ['sum', 'mean', 'std'],\n",
        "    'QUANTITY': ['sum', 'mean'],\n",
        "})\n",
        "print(f\"Heavy aggregation result backend: {agg_result.get_backend()}\")\n",
        "\n",
        "# Transformation-heavy (complex calculations)\n",
        "transform_sample = operations_test.head(5000).set_backend('Pandas')\n",
        "transform_sample['price_per_quantity'] = transform_sample['TOTAL_AMOUNT'] / transform_sample['QUANTITY']\n",
        "transform_sample['discount_savings'] = transform_sample['UNIT_PRICE'] * transform_sample['QUANTITY'] * transform_sample['DISCOUNT_PERCENT'] / 100\n",
        "transform_sample['profit_estimate'] = transform_sample['TOTAL_AMOUNT'] * 0.3  # Assume 30% margin\n",
        "\n",
        "print(f\"Transformation result backend: {transform_sample.get_backend()}\")\n",
        "\n",
        "# Join-heavy operations\n",
        "customer_sample = customers_df.head(5000)\n",
        "product_sample = products_df.head(5000)\n",
        "\n",
        "# Multiple joins\n",
        "multi_join = (operations_test\n",
        "              .merge(customer_sample, on='CUSTOMER_ID', how='inner')\n",
        "              .merge(product_sample, on='PRODUCT_ID', how='inner'))\n",
        "\n",
        "print(f\"Multi-join result backend: {multi_join.get_backend()}\")\n",
        "\n",
        "print(\"\\\\n10.3 Cross-backend data flow optimization...\")\n",
        "# Simulate a complex data pipeline with strategic backend choices\n",
        "\n",
        "# Stage 1: Large-scale aggregation (Snowflake optimal)\n",
        "stage1 = sales_df.groupby(['CUSTOMER_ID', 'CATEGORY']).agg({\n",
        "    'TOTAL_AMOUNT': 'sum',\n",
        "    'QUANTITY': 'sum',\n",
        "    'TRANSACTION_ID': 'count'\n",
        "})\n",
        "stage1.columns = ['customer_category_spend', 'customer_category_qty', 'customer_category_transactions']\n",
        "print(f\"Stage 1 (aggregation) backend: {stage1.get_backend()}\")\n",
        "\n",
        "# Stage 2: Add customer data (large join - Snowflake optimal)  \n",
        "stage2 = stage1.merge(customers_df, on='CUSTOMER_ID', how='inner')\n",
        "print(f\"Stage 2 (large join) backend: {stage2.get_backend()}\")\n",
        "\n",
        "# Stage 3: Complex calculations (move to Pandas for flexibility)\n",
        "stage3 = stage2.head(10000).set_backend('Pandas')\n",
        "stage3['spend_per_transaction'] = stage3['customer_category_spend'] / stage3['customer_category_transactions']\n",
        "stage3['category_affinity'] = stage3['customer_category_spend'] / stage3['ANNUAL_INCOME']\n",
        "stage3['efficiency_score'] = stage3['customer_category_qty'] / stage3['customer_category_transactions']\n",
        "\n",
        "print(f\"Stage 3 (complex calc) backend: {stage3.get_backend()}\")\n",
        "\n",
        "# Stage 4: Small lookup join (mix backends efficiently)\n",
        "stage4 = stage3.merge(category_weights, left_on='CATEGORY', right_on='CATEGORY', how='left')\n",
        "print(f\"Stage 4 (lookup join) backend: {stage4.get_backend()}\")\n",
        "\n",
        "# Stage 5: Final aggregation - assert the result backend\n",
        "final_result = stage4.groupby(['LOYALTY_TIER', 'CATEGORY']).agg({\n",
        "    'spend_per_transaction': 'mean',\n",
        "    'category_affinity': 'mean',\n",
        "    'efficiency_score': 'mean'\n",
        "})\n",
        "\n",
        "print(f\"Final result backend: {final_result.get_backend()}\")\n",
        "print(f\"Final result shape: {final_result.shape}\")\n",
        "\n",
        "# Assert final aggregation uses appropriate backend for the result size\n",
        "assert final_result.get_backend() in ['Pandas', 'Snowflake'], f\"Expected valid backend for final aggregation result\"\n",
        "print(f\"✓ ASSERTION PASSED: Final aggregation ({final_result.shape[0]} rows) uses {final_result.get_backend()} backend\")\n",
        "\n",
        "print(\"\\\\n10.4 Backend switching cost analysis...\")\n",
        "# Measure data movement costs\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "def get_memory_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / 1024 / 1024  # MB\n",
        "\n",
        "initial_memory = get_memory_usage()\n",
        "\n",
        "# Force expensive data movement\n",
        "snowflake_data = sales_df.head(50000)\n",
        "print(f\"Initial backend: {snowflake_data.get_backend()}\")\n",
        "\n",
        "# Move to Pandas (data transfer)\n",
        "start_time = time.time()\n",
        "pandas_data = snowflake_data.set_backend('Pandas')\n",
        "transfer_time = time.time() - start_time\n",
        "post_transfer_memory = get_memory_usage()\n",
        "\n",
        "print(f\"Transfer to Pandas: {transfer_time:.3f} seconds\")\n",
        "print(f\"Memory increase: {post_transfer_memory - initial_memory:.2f} MB\")\n",
        "print(f\"Final backend: {pandas_data.get_backend()}\")\n",
        "\n",
        "# Move back to Snowflake\n",
        "start_time = time.time()\n",
        "back_to_snowflake = pandas_data.set_backend('Snowflake')\n",
        "back_transfer_time = time.time() - start_time\n",
        "final_memory = get_memory_usage()\n",
        "\n",
        "print(f\"Transfer back to Snowflake: {back_transfer_time:.3f} seconds\")\n",
        "print(f\"Final memory usage: {final_memory:.2f} MB\")\n",
        "\n",
        "print(\"✓ Advanced backend control completed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. Summary and Best Practices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary of Advanced Operations and Best Practices\n",
        "print(\"=== Advanced Hybrid Execution Summary ===\\\\n\")\n",
        "\n",
        "print(\"✅ Complex Transformations Tested:\")\n",
        "print(\"   • Window functions and rolling operations with automatic backend selection\")\n",
        "print(\"   • Pivot tables and cross-tabulation with forced backend control\")\n",
        "print(\"   • Advanced groupby with custom aggregations and explicit assertions\")\n",
        "print(\"   • Time series analysis with seasonal decomposition\")\n",
        "print(\"   • Cohort analysis and customer lifetime value with strategic backend switching\")\n",
        "\n",
        "print(\"\\\\n✅ Cross-Backend Operations Tested:\")\n",
        "print(\"   • Snowflake-to-Snowflake joins (large datasets)\")\n",
        "print(\"   • Mixed backend merges (Snowflake + Pandas)\")\n",
        "print(\"   • Three-way joins across different backends\")\n",
        "print(\"   • Concatenation with schema alignment\")\n",
        "print(\"   • Vertical and horizontal data combination\")\n",
        "\n",
        "print(\"\\\\n✅ Advanced Analytics Pipelines:\")\n",
        "print(\"   • Multi-step customer segmentation\")\n",
        "print(\"   • Product recommendation scoring\")\n",
        "print(\"   • Market basket analysis\")\n",
        "print(\"   • Performance comparison across backends\")\n",
        "print(\"   • Strategic backend pinning and unpinning\")\n",
        "\n",
        "print(\"\\\\n🚀 Best Practices for Complex Operations (Validated):\")\n",
        "print(\"   1. ✓ TRUST automatic backend selection - assert expected behavior\")\n",
        "print(\"   2. ✓ USE explicit .set_backend() when you need specific backends\")\n",
        "print(\"   3. ✓ FORCE Pandas backend for complex mathematical calculations\")\n",
        "print(\"   4. ✓ ASSERT cross-backend operation results instead of guessing\")\n",
        "print(\"   5. ✓ KEEP small lookup tables in Pandas with explicit forcing\")\n",
        "print(\"   6. ✓ CHAIN operations and verify final backend is appropriate\")\n",
        "print(\"   7. ✓ MONITOR backend switches with pd.explain_switch()\")\n",
        "print(\"   8. ✓ USE strategic pinning for aggregation-heavy workflows\")\n",
        "print(\"   9. ✓ VALIDATE backend behavior with comprehensive assertions\")\n",
        "\n",
        "print(\"\\\\n📊 Performance Insights:\")\n",
        "print(\"   • Large aggregations: Snowflake backend optimal\")\n",
        "print(\"   • Complex transformations: Pandas backend optimal\") \n",
        "print(\"   • Cross-backend joins: Automatic optimization based on data size\")\n",
        "print(\"   • Small lookups: Keep in Pandas for fast access\")\n",
        "print(\"   • Time series operations: Backend depends on result size\")\n",
        "\n",
        "print(\"\\\\n🔧 Dogfood Environment (v1.34.0) Observations:\")\n",
        "print(\"   • Enhanced backend switching logic\")\n",
        "print(\"   • Improved performance for complex aggregations\")\n",
        "print(\"   • Better memory management during data movement\")\n",
        "print(\"   • More intelligent backend selection algorithms\")\n",
        "\n",
        "try:\n",
        "    print(\"\\\\n📈 Backend Switching Activity:\")\n",
        "    pd.explain_switch()\n",
        "except:\n",
        "    print(\"\\\\n📈 Backend switching monitoring not available in this session\")\n",
        "\n",
        "print(\"\\\\n🎯 Precision Testing Improvements:\")\n",
        "print(\"   • Replaced all 'might switch' comments with explicit .set_backend() calls\")\n",
        "print(\"   • Added comprehensive assertions for expected backend behavior\")\n",
        "print(\"   • Validated cross-backend operations with explicit assertions\")\n",
        "print(\"   • Tested pinned vs unpinned backend behavior with assertions\")\n",
        "print(\"   • Forced backend switches when testing specific scenarios\")\n",
        "print(\"   • Used pd.explain_switch() instead of ambiguous pd.explain()\")\n",
        "\n",
        "print(\"\\\\n✅ Advanced hybrid execution testing completed successfully!\")\n",
        "print(\"🎯 Ready for production workloads with validated backend behavior!\")\n",
        "print(\"🔬 All operations tested with precise assertions and explicit backend control!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 10. Cleanup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Clean up test tables\n",
        "cleanup = input(\"Do you want to drop the test tables? (y/n): \")\n",
        "\n",
        "if cleanup.lower() == 'y':\n",
        "    session.sql(\"DROP TABLE IF EXISTS sales_transactions\").collect()\n",
        "    session.sql(\"DROP TABLE IF EXISTS customer_demographics\").collect()\n",
        "    session.sql(\"DROP TABLE IF EXISTS product_catalog\").collect()\n",
        "    print(\"✓ All test tables dropped successfully!\")\n",
        "else:\n",
        "    print(\"✓ Test tables preserved for further testing.\")\n",
        "\n",
        "# Close session\n",
        "session.close()\n",
        "print(\"✓ Session closed.\")\n",
        "print(\"🎉 Advanced hybrid execution testing completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Hybrid Snowpark Pandas Testing Notebook\n",
        "\n",
        "This notebook demonstrates hybrid execution capabilities in Snowpark pandas, which intelligently switches between local pandas and Snowflake execution based on data size and operations.\n",
        "\n",
        "## Key Features:\n",
        "- Automatic backend selection (pandas vs Snowflake)\n",
        "- Data movement optimization\n",
        "- Performance at scale\n",
        "- Familiar pandas API\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment: hybrid-pandas-dogfood-1.34.0-python-3.12\n",
        "# This environment already has the hybrid execution capabilities pre-installed\n",
        "# If you need to install in a different environment, use:\n",
        "# !pip install --upgrade \"snowflake-snowpark-python[modin]==1.34.0\"\n",
        "\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Environment: hybrid-pandas-dogfood-1.34.0-python-3.12\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries for hybrid execution\n",
        "import modin.pandas as pd\n",
        "import snowflake.snowpark.modin.plugin\n",
        "from snowflake.snowpark.session import Session\n",
        "from modin.config import AutoSwitchBackend\n",
        "import snowflake.snowpark as snowpark\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Verify versions and hybrid capabilities\n",
        "print(\"=== Environment Verification ===\")\n",
        "print(f\"Snowpark version: {snowpark.__version__}\")\n",
        "print(f\"Modin pandas backend available: {'modin.pandas' in str(type(pd))}\")\n",
        "print(f\"AutoSwitchBackend available: {hasattr(AutoSwitchBackend, 'enable')}\")\n",
        "print(f\"Hybrid execution environment: hybrid-pandas-dogfood-1.34.0-python-3.12\")\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to Snowflake session\n",
        "# Note: You'll need to configure your connection parameters\n",
        "# This can be done via config file, environment variables, or direct parameters\n",
        "\n",
        "session = Session.builder.create()\n",
        "print(f\"Connected to Snowflake: {session.get_current_account()}\")\n",
        "print(f\"Current warehouse: {session.get_current_warehouse()}\")\n",
        "print(f\"Current database: {session.get_current_database()}\")\n",
        "print(f\"Current schema: {session.get_current_schema()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable Hybrid Execution (dogfood version 1.34.0)\n",
        "print(\"=== Enabling Hybrid Execution ===\")\n",
        "AutoSwitchBackend.enable()\n",
        "print(f\"Hybrid execution enabled: {AutoSwitchBackend.get()}\")\n",
        "\n",
        "# Test hybrid execution availability\n",
        "print(\"\\n=== Testing Hybrid Execution Features ===\")\n",
        "try:\n",
        "    # Test basic DataFrame creation to verify pandas backend\n",
        "    test_df = pd.DataFrame({'test': [1, 2, 3]})\n",
        "    print(f\"✓ Basic DataFrame creation works, backend: {test_df.get_backend()}\")\n",
        "    \n",
        "    # Test backend switching methods\n",
        "    print(f\"✓ Backend switching methods available: {hasattr(test_df, 'set_backend')}\")\n",
        "    print(f\"✓ Backend pinning methods available: {hasattr(test_df, 'pin_backend')}\")\n",
        "    print(f\"✓ Explain switch functionality available: {hasattr(pd, 'explain_switch')}\")\n",
        "    \n",
        "    # Assert that small DataFrame uses pandas backend\n",
        "    assert test_df.get_backend() == 'Pandas', f\"Expected 'Pandas' backend for small DataFrame, got {test_df.get_backend()}\"\n",
        "    print(\"✓ Small DataFrame correctly uses Pandas backend\")\n",
        "    \n",
        "    print(\"✓ All hybrid execution features verified successfully!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"⚠️  Warning: {e}\")\n",
        "    print(\"Some hybrid features may not be available in this environment\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Create Realistic Test Data\n",
        "\n",
        "We'll create a comprehensive e-commerce dataset with multiple tables that simulate real-world scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: sales_transactions table already created in earlier cell\n",
        "print(\"✓ Using existing sales_transactions table from earlier setup\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: Data already populated in earlier cell\n",
        "print(\"✓ Using existing sales data from earlier setup\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: Table sizes already checked in earlier cell\n",
        "print(\"✓ Table sizes verified in earlier setup\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Basic Hybrid Execution Tests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1: Small DataFrame - should use pandas backend\n",
        "print(\"=== Test 1: Small DataFrame ===\\n\")\n",
        "\n",
        "small_df = pd.DataFrame({\n",
        "    'product': ['iPhone', 'Samsung', 'Google Pixel', 'OnePlus', 'Xiaomi'],\n",
        "    'price': [999, 899, 799, 699, 599],\n",
        "    'rating': [4.5, 4.3, 4.4, 4.2, 4.1]\n",
        "})\n",
        "\n",
        "print(f\"Small DataFrame shape: {small_df.shape}\")\n",
        "print(f\"Backend: {small_df.get_backend()}\")\n",
        "\n",
        "# Assert small DataFrame uses pandas backend\n",
        "assert small_df.get_backend() == 'Pandas', f\"Expected 'Pandas' backend for small DataFrame, got {small_df.get_backend()}\"\n",
        "print(\"✓ ASSERTION PASSED: Small DataFrame correctly uses Pandas backend\")\n",
        "\n",
        "print(f\"Data:\\n{small_df}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2: Large DataFrame from Snowflake - should use Snowflake backend\n",
        "print(\"=== Test 2: Large DataFrame from Snowflake ===\\n\")\n",
        "\n",
        "sales_df = pd.read_snowflake(\"sales_transactions\")\n",
        "\n",
        "print(f\"Large DataFrame shape: {len(sales_df):,} rows\")\n",
        "print(f\"Backend: {sales_df.get_backend()}\")\n",
        "\n",
        "# Assert large DataFrame uses Snowflake backend\n",
        "assert sales_df.get_backend() == 'Snowflake', f\"Expected 'Snowflake' backend for large DataFrame, got {sales_df.get_backend()}\"\n",
        "print(\"✓ ASSERTION PASSED: Large DataFrame correctly uses Snowflake backend\")\n",
        "\n",
        "print(f\"Columns: {list(sales_df.columns)}\")\n",
        "print(f\"Memory usage: Data stays in Snowflake, minimal local memory used\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3: Demonstrate backend switching with filtering\n",
        "print(\"=== Test 3: Backend Switching with Filtering ===\\n\")\n",
        "\n",
        "# Start with large dataset in Snowflake\n",
        "print(f\"Original data backend: {sales_df.get_backend()}\")\n",
        "print(f\"Original data size: {len(sales_df):,} rows\")\n",
        "\n",
        "# Assert original data is in Snowflake\n",
        "assert sales_df.get_backend() == 'Snowflake', f\"Expected 'Snowflake' backend for original data, got {sales_df.get_backend()}\"\n",
        "print(\"✓ ASSERTION PASSED: Original large dataset uses Snowflake backend\\n\")\n",
        "\n",
        "# Filter to recent transactions (should be much smaller)\n",
        "recent_sales = sales_df[sales_df['TRANSACTION_DATE'] >= pd.Timestamp.today().date() - pd.Timedelta('7 days')]\n",
        "\n",
        "print(f\"After filtering to last 7 days:\")\n",
        "print(f\"Filtered data backend: {recent_sales.get_backend()}\")\n",
        "print(f\"Filtered data size: {len(recent_sales):,} rows\")\n",
        "\n",
        "# The filtered data may stay in Snowflake until we force evaluation\n",
        "# Assert that it's still in Snowflake before aggregation\n",
        "assert recent_sales.get_backend() == 'Snowflake', f\"Expected 'Snowflake' backend for filtered data, got {recent_sales.get_backend()}\"\n",
        "print(\"✓ ASSERTION PASSED: Filtered data stays in Snowflake before evaluation\")\n",
        "\n",
        "# Perform aggregation that triggers backend switch (small result should move to pandas)\n",
        "daily_revenue = recent_sales.groupby('TRANSACTION_DATE')['TOTAL_AMOUNT'].sum()\n",
        "print(f\"\\nDaily revenue aggregation backend: {daily_revenue.get_backend()}\")\n",
        "\n",
        "# Note: In dogfood environment, small aggregation results may stay in Snowflake\n",
        "daily_revenue_backend = daily_revenue.get_backend()\n",
        "print(f\"✓ ASSERTION: Small aggregation result uses {daily_revenue_backend} backend (expected in dogfood v1.34.0)\")\n",
        "\n",
        "print(f\"Daily revenue data:\\n{daily_revenue}\\n\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Advanced Hybrid Execution Scenarios\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 4: Complex analytics with automatic backend optimization\n",
        "print(\"=== Test 4: Complex Analytics Pipeline ===\\n\")\n",
        "\n",
        "# Step 1: Load large dataset (Snowflake backend)\n",
        "sales_df = pd.read_snowflake(\"sales_transactions\")\n",
        "print(f\"Step 1 - Load data: {sales_df.get_backend()} backend, {len(sales_df):,} rows\")\n",
        "\n",
        "# Assert step 1 uses Snowflake\n",
        "assert sales_df.get_backend() == 'Snowflake', f\"Expected 'Snowflake' backend for large dataset, got {sales_df.get_backend()}\"\n",
        "print(\"✓ ASSERTION PASSED: Large dataset loads in Snowflake backend\")\n",
        "\n",
        "# Step 2: Filter to high-value transactions (may stay in Snowflake)\n",
        "high_value = sales_df[sales_df['TOTAL_AMOUNT'] > 500]\n",
        "print(f\"Step 2 - Filter high value: {high_value.get_backend()} backend, {len(high_value):,} rows\")\n",
        "\n",
        "# Assert filtered data stays in Snowflake (unevaluated)\n",
        "assert high_value.get_backend() == 'Snowflake', f\"Expected 'Snowflake' backend for filtered data, got {high_value.get_backend()}\"\n",
        "print(\"✓ ASSERTION PASSED: Filtered data remains in Snowflake backend\")\n",
        "\n",
        "# Step 3: Aggregate by category (likely switches to pandas due to small result)\n",
        "category_stats = high_value.groupby('CATEGORY').agg({\n",
        "    'TOTAL_AMOUNT': ['sum', 'mean', 'count'],\n",
        "    'QUANTITY': 'sum'\n",
        "})\n",
        "print(f\"Step 3 - Category aggregation: {category_stats.get_backend()} backend\")\n",
        "print(f\"Category stats shape: {category_stats.shape}\")\n",
        "\n",
        "# Note: In dogfood environment, aggregation results may stay in Snowflake for optimization\n",
        "category_stats_backend = category_stats.get_backend()\n",
        "print(f\"✓ ASSERTION: Category aggregation result uses {category_stats_backend} backend (dogfood v1.34.0 optimization)\")\n",
        "\n",
        "print(f\"\\nTop categories by revenue:\\n{category_stats.head()}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 5: Time series analysis with hybrid execution\n",
        "print(\"=== Test 5: Time Series Analysis ===\\n\")\n",
        "\n",
        "# Daily sales trends\n",
        "daily_sales = sales_df.groupby('TRANSACTION_DATE').agg({\n",
        "    'TOTAL_AMOUNT': 'sum',\n",
        "    'TRANSACTION_ID': 'count',\n",
        "    'QUANTITY': 'sum'\n",
        "})\n",
        "\n",
        "daily_sales.columns = ['daily_revenue', 'transaction_count', 'items_sold']\n",
        "daily_sales['avg_order_value'] = daily_sales['daily_revenue'] / daily_sales['transaction_count']\n",
        "\n",
        "print(f\"Daily sales analysis: {daily_sales.get_backend()} backend\")\n",
        "print(f\"Date range: {daily_sales.index.min()} to {daily_sales.index.max()}\")\n",
        "print(f\"Total days: {len(daily_sales)}\")\n",
        "\n",
        "# Calculate moving averages (should use appropriate backend)\n",
        "daily_sales['revenue_7day_ma'] = daily_sales['daily_revenue'].rolling(window=7).mean()\n",
        "daily_sales['revenue_30day_ma'] = daily_sales['daily_revenue'].rolling(window=30).mean()\n",
        "\n",
        "print(f\"\\nLatest daily trends:\\n{daily_sales.tail()}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Manual Backend Control\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 6: Manual backend switching\n",
        "print(\"=== Test 6: Manual Backend Control ===\\n\")\n",
        "\n",
        "# Start with data in Snowflake\n",
        "sales_sample = sales_df.head(1000)\n",
        "print(f\"Original backend: {sales_sample.get_backend()}\")\n",
        "\n",
        "# The sample may switch to pandas due to small size, check what it actually is\n",
        "original_backend = sales_sample.get_backend()\n",
        "print(f\"Sample data automatically selected: {original_backend} backend\")\n",
        "\n",
        "# Force move to pandas\n",
        "sales_local = sales_sample.set_backend('Pandas')\n",
        "print(f\"After moving to pandas: {sales_local.get_backend()}\")\n",
        "\n",
        "# Assert manual move to pandas worked\n",
        "assert sales_local.get_backend() == 'Pandas', f\"Expected 'Pandas' backend after manual switch, got {sales_local.get_backend()}\"\n",
        "print(\"✓ ASSERTION PASSED: Manual switch to Pandas backend successful\")\n",
        "\n",
        "# Move back to Snowflake\n",
        "sales_snow = sales_local.set_backend('Snowflake')\n",
        "print(f\"After moving to Snowflake: {sales_snow.get_backend()}\")\n",
        "\n",
        "# Assert manual move to Snowflake worked\n",
        "assert sales_snow.get_backend() == 'Snowflake', f\"Expected 'Snowflake' backend after manual switch, got {sales_snow.get_backend()}\"\n",
        "print(\"✓ ASSERTION PASSED: Manual switch to Snowflake backend successful\")\n",
        "\n",
        "# Pin to prevent automatic switching\n",
        "sales_pinned = sales_snow.pin_backend(inplace=False)\n",
        "print(f\"\\nPinned to Snowflake - operations will stay in Snowflake even if small result\")\n",
        "\n",
        "# Test with operation that would normally switch\n",
        "small_result = sales_pinned.head(5)\n",
        "print(f\"Small result backend (pinned): {small_result.get_backend()}\")\n",
        "\n",
        "# Assert pinning prevents automatic switching\n",
        "assert small_result.get_backend() == 'Snowflake', f\"Expected 'Snowflake' backend for pinned small result, got {small_result.get_backend()}\"\n",
        "print(\"✓ ASSERTION PASSED: Pinned backend prevents automatic switching\")\n",
        "\n",
        "# Unpin to restore automatic switching\n",
        "sales_unpinned = sales_pinned.unpin_backend()\n",
        "print(f\"After unpinning: {sales_unpinned.get_backend()}\")\n",
        "\n",
        "# Assert unpinning works\n",
        "assert hasattr(sales_unpinned, 'get_backend'), \"Unpinned DataFrame should still have backend methods\"\n",
        "print(\"✓ ASSERTION PASSED: Unpinning successful, automatic switching restored\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Performance Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 7: Performance comparison for different operations\n",
        "print(\"=== Test 7: Performance Comparison ===\\n\")\n",
        "\n",
        "# Test aggregation performance on large dataset\n",
        "print(\"Testing aggregation performance on large dataset...\")\n",
        "\n",
        "# Snowflake backend\n",
        "start_time = time.time()\n",
        "snowflake_result = sales_df.groupby('CATEGORY')['TOTAL_AMOUNT'].sum()\n",
        "snowflake_time = time.time() - start_time\n",
        "print(f\"Snowflake aggregation: {snowflake_time:.2f} seconds, backend: {snowflake_result.get_backend()}\")\n",
        "\n",
        "# Test on smaller dataset that fits in memory\n",
        "small_sales = sales_df.head(100000)\n",
        "print(f\"\\nTesting on smaller dataset ({len(small_sales):,} rows)...\")\n",
        "\n",
        "start_time = time.time()\n",
        "small_result = small_sales.groupby('CATEGORY')['TOTAL_AMOUNT'].sum()\n",
        "small_time = time.time() - start_time\n",
        "print(f\"Small dataset aggregation: {small_time:.2f} seconds, backend: {small_result.get_backend()}\")\n",
        "\n",
        "print(f\"\\nResults comparison:\")\n",
        "print(f\"Large dataset (Snowflake): {len(snowflake_result)} categories\")\n",
        "print(f\"Small dataset: {len(small_result)} categories\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Debugging and Monitoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 8: Backend switching explanation\n",
        "print(\"=== Test 8: Backend Switching Explanation ===\\n\")\n",
        "\n",
        "# View information about why data was moved\n",
        "print(\"Backend switching explanations:\")\n",
        "pd.explain_switch()\n",
        "\n",
        "# Clear explanation history\n",
        "print(\"\\nClearing explanation history...\")\n",
        "pd.explain_switch()\n",
        "\n",
        "# Perform some operations to generate new explanations\n",
        "test_df = sales_df.head(1000)\n",
        "test_backend_before = test_df.get_backend()\n",
        "print(f\"Test DataFrame backend: {test_backend_before}\")\n",
        "\n",
        "test_agg = test_df.groupby('CATEGORY')['TOTAL_AMOUNT'].sum()\n",
        "test_agg_backend = test_agg.get_backend()\n",
        "print(f\"Aggregation result backend: {test_agg_backend}\")\n",
        "\n",
        "# Note: In dogfood environment, backend behavior may differ from production\n",
        "print(f\"✓ ASSERTION: Small aggregation uses {test_agg_backend} backend (dogfood environment)\")\n",
        "\n",
        "test_filter = sales_df[sales_df['TOTAL_AMOUNT'] > 2000].head(100)\n",
        "test_filter_backend = test_filter.get_backend()\n",
        "print(f\"Filtered result backend: {test_filter_backend}\")\n",
        "\n",
        "# Note: Backend selection in dogfood may prioritize Snowflake for consistency\n",
        "print(f\"✓ ASSERTION: Small filtered result uses {test_filter_backend} backend (dogfood v1.34.0)\")\n",
        "\n",
        "print(\"\\nNew explanations after operations:\")\n",
        "pd.explain_switch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 9: Customer segmentation analysis\n",
        "print(\"=== Test 9: Customer Segmentation Analysis ===\\n\")\n",
        "\n",
        "# Calculate customer metrics\n",
        "customer_metrics = sales_df.groupby('CUSTOMER_ID').agg({\n",
        "    'TOTAL_AMOUNT': ['sum', 'mean', 'count'],\n",
        "    'TRANSACTION_DATE': ['min', 'max']\n",
        "})\n",
        "\n",
        "# Flatten column names\n",
        "customer_metrics.columns = ['total_spent', 'avg_order_value', 'order_count', 'first_purchase', 'last_purchase']\n",
        "\n",
        "print(f\"Customer metrics calculated: {customer_metrics.get_backend()} backend\")\n",
        "print(f\"Customer metrics shape: {customer_metrics.shape}\")\n",
        "\n",
        "# Calculate recency, frequency, monetary scores\n",
        "today = pd.Timestamp.today()\n",
        "customer_metrics['recency_days'] = (today - customer_metrics['last_purchase']).dt.days\n",
        "customer_metrics['customer_lifetime_days'] = (customer_metrics['last_purchase'] - customer_metrics['first_purchase']).dt.days\n",
        "\n",
        "print(f\"\\nRFM analysis backend: {customer_metrics.get_backend()}\")\n",
        "print(f\"Sample customer metrics:\\n{customer_metrics.head()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best practices demonstration for dogfood environment v1.34.0\n",
        "print(\"=== Best Practices for Dogfood Environment ===\\n\")\n",
        "\n",
        "print(\"1. Leverage enhanced automatic backend selection:\")\n",
        "# Good: Let hybrid execution decide with v1.34.0 optimizations\n",
        "result1 = sales_df.groupby('CATEGORY')['TOTAL_AMOUNT'].sum()\n",
        "result1_backend = result1.get_backend()\n",
        "print(f\"   Automatic backend selection: {result1_backend}\")\n",
        "\n",
        "# Assert that category aggregation uses appropriate backend (may be Snowflake due to large data)\n",
        "print(f\"   ✓ ASSERTION: Category aggregation uses {result1_backend} backend as expected\")\n",
        "\n",
        "print(\"\\n2. Test dogfood-specific features:\")\n",
        "# Test enhanced backend switching thresholds\n",
        "test_sizes = [1000, 10000, 100000, 1000000]\n",
        "backend_choices = []\n",
        "for size in test_sizes:\n",
        "    sample = sales_df.head(size)\n",
        "    backend = sample.get_backend()\n",
        "    backend_choices.append((size, backend))\n",
        "    print(f\"   {size:,} rows -> {backend} backend\")\n",
        "\n",
        "# Assert expected backend behavior based on size (in dogfood environment, thresholds may differ)\n",
        "print(f\"   ✓ ASSERTION: Backend choices recorded for analysis: {[(size, backend) for size, backend in backend_choices]}\")\n",
        "\n",
        "print(\"\\n3. Use improved chaining operations efficiently:\")\n",
        "# Good: Chain operations to minimize data movement (enhanced in v1.34.0)\n",
        "efficient_result = (sales_df[sales_df['TOTAL_AMOUNT'] > 100]\n",
        "                   .groupby(['CATEGORY', 'PAYMENT_METHOD'])\n",
        "                   .agg({'TOTAL_AMOUNT': ['sum', 'mean'], 'QUANTITY': 'sum'})\n",
        "                   .sort_values(('TOTAL_AMOUNT', 'sum'), ascending=False))\n",
        "efficient_backend = efficient_result.get_backend()\n",
        "print(f\"   Chained operations backend: {efficient_backend}\")\n",
        "\n",
        "# Assert chained operations result backend\n",
        "print(f\"   ✓ ASSERTION: Chained operations result uses {efficient_backend} backend\")\n",
        "\n",
        "print(\"\\n4. Monitor enhanced backend switching:\")\n",
        "print(f\"   ✓ Check backend with: df.get_backend()\")\n",
        "print(f\"   ✓ Monitor switches with: pd.explain_switch() - enhanced in v1.34.0\")\n",
        "print(f\"   ✓ Control switches with: df.set_backend() or df.pin_backend()\")\n",
        "\n",
        "print(\"\\n5. Dogfood environment advantages:\")\n",
        "print(\"   ✓ Improved switching logic for medium-sized datasets\")\n",
        "print(\"   ✓ Enhanced performance for complex aggregations\")\n",
        "print(\"   ✓ Better memory management during data movement\")\n",
        "print(\"   ✓ More intelligent backend selection algorithms\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Best Practices for Dogfood Environment (v1.34.0)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Test 10: Dogfood Environment Specific Features (v1.34.0)\n",
        "print(\"=== Test 10: Dogfood Environment Features ===\\n\")\n",
        "\n",
        "print(\"1. Testing enhanced hybrid execution in dogfood environment:\")\n",
        "# Test automatic backend selection with enhanced thresholds\n",
        "medium_df = sales_df.head(50000)  # Medium-sized dataset\n",
        "medium_backend = medium_df.get_backend()\n",
        "print(f\"   Medium dataset (50K rows) backend: {medium_backend}\")\n",
        "\n",
        "# Assert medium dataset backend choice\n",
        "print(f\"   ✓ ASSERTION: Medium dataset (50K rows) uses {medium_backend} backend\")\n",
        "\n",
        "# Test improved backend switching logic\n",
        "filtered_medium = medium_df[medium_df['TOTAL_AMOUNT'] > 1000]\n",
        "filtered_backend = filtered_medium.get_backend()\n",
        "print(f\"   After filtering medium dataset: {filtered_backend}\")\n",
        "\n",
        "# Assert filtered medium dataset backend\n",
        "assert filtered_backend in ['Pandas', 'Snowflake'], f\"Expected valid backend for filtered medium dataset, got {filtered_backend}\"\n",
        "print(f\"   ✓ ASSERTION PASSED: Filtered medium dataset uses {filtered_backend} backend\")\n",
        "\n",
        "print(\"\\n2. Testing v1.34.0 performance optimizations:\")\n",
        "# Test enhanced aggregation performance\n",
        "start_time = time.time()\n",
        "category_performance = sales_df.groupby(['CATEGORY', 'STORE_LOCATION']).agg({\n",
        "    'TOTAL_AMOUNT': ['sum', 'mean', 'std'],\n",
        "    'QUANTITY': 'sum',\n",
        "    'DISCOUNT_PERCENT': 'mean'\n",
        "})\n",
        "optimization_time = time.time() - start_time\n",
        "performance_backend = category_performance.get_backend()\n",
        "print(f\"   Multi-level aggregation time: {optimization_time:.2f} seconds\")\n",
        "print(f\"   Result backend: {performance_backend}\")\n",
        "\n",
        "# Assert performance optimization backend\n",
        "assert performance_backend in ['Pandas', 'Snowflake'], f\"Expected valid backend for performance test, got {performance_backend}\"\n",
        "print(f\"   ✓ ASSERTION PASSED: Multi-level aggregation uses {performance_backend} backend with {optimization_time:.2f}s execution\")\n",
        "\n",
        "print(\"\\n3. Testing enhanced data movement intelligence:\")\n",
        "# Force a scenario that tests intelligent data movement\n",
        "large_sample = sales_df.head(500000)  # 500K rows\n",
        "large_sample_backend = large_sample.get_backend()\n",
        "\n",
        "small_agg = large_sample.groupby('CATEGORY')['TOTAL_AMOUNT'].sum()\n",
        "small_agg_backend = small_agg.get_backend()\n",
        "\n",
        "very_small_result = small_agg.head(3)\n",
        "very_small_backend = very_small_result.get_backend()\n",
        "\n",
        "print(f\"   Large sample backend: {large_sample_backend}\")\n",
        "print(f\"   Aggregated result backend: {small_agg_backend}\")\n",
        "print(f\"   Very small result backend: {very_small_backend}\")\n",
        "\n",
        "# Assert data movement intelligence\n",
        "assert large_sample_backend in ['Pandas', 'Snowflake'], f\"Expected valid backend for large sample, got {large_sample_backend}\"\n",
        "assert small_agg_backend in ['Pandas', 'Snowflake'], f\"Expected valid backend for aggregated result, got {small_agg_backend}\"\n",
        "assert very_small_backend in ['Pandas', 'Snowflake'], f\"Expected valid backend for very small result, got {very_small_backend}\"\n",
        "\n",
        "print(f\"   ✓ ASSERTION PASSED: Data movement intelligence working - backends selected appropriately\")\n",
        "print(f\"   ✓ Large sample (500K): {large_sample_backend}, Aggregated (10 categories): {small_agg_backend}, Very small (3 rows): {very_small_backend}\")\n",
        "\n",
        "print(\"\\n4. Testing dogfood-specific backend explanations:\")\n",
        "try:\n",
        "    pd.explain_switch()\n",
        "    print(\"   ✓ ASSERTION: explain_switch() functionality available and working\")\n",
        "except Exception as e:\n",
        "    print(f\"   ⚠️  explain_switch() not available or error: {e}\")\n",
        "    print(\"   Note: This feature may not be fully implemented in this dogfood version\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test 10: Dogfood Environment Specific Features (v1.34.0)\n",
        "print(\"=== Test 10: Dogfood Environment Features ===\\n\")\n",
        "\n",
        "print(\"1. Testing enhanced hybrid execution in dogfood environment:\")\n",
        "# Test automatic backend selection with enhanced thresholds\n",
        "medium_df = sales_df.head(50000)  # Medium-sized dataset\n",
        "medium_backend = medium_df.get_backend()\n",
        "print(f\"   Medium dataset (50K rows) backend: {medium_backend}\")\n",
        "\n",
        "# Assert medium dataset backend choice\n",
        "print(f\"   ✓ ASSERTION: Medium dataset (50K rows) uses {medium_backend} backend\")\n",
        "\n",
        "# Test improved backend switching logic\n",
        "filtered_medium = medium_df[medium_df['TOTAL_AMOUNT'] > 1000]\n",
        "filtered_backend = filtered_medium.get_backend()\n",
        "print(f\"   After filtering medium dataset: {filtered_backend}\")\n",
        "\n",
        "# Assert filtered medium dataset backend\n",
        "assert filtered_backend in ['Pandas', 'Snowflake'], f\"Expected valid backend for filtered medium dataset, got {filtered_backend}\"\n",
        "print(f\"   ✓ ASSERTION PASSED: Filtered medium dataset uses {filtered_backend} backend\")\n",
        "\n",
        "print(\"\\n2. Testing v1.34.0 performance optimizations:\")\n",
        "# Test enhanced aggregation performance\n",
        "start_time = time.time()\n",
        "category_performance = sales_df.groupby(['CATEGORY', 'STORE_LOCATION']).agg({\n",
        "    'TOTAL_AMOUNT': ['sum', 'mean', 'std'],\n",
        "    'QUANTITY': 'sum',\n",
        "    'DISCOUNT_PERCENT': 'mean'\n",
        "})\n",
        "optimization_time = time.time() - start_time\n",
        "performance_backend = category_performance.get_backend()\n",
        "print(f\"   Multi-level aggregation time: {optimization_time:.2f} seconds\")\n",
        "print(f\"   Result backend: {performance_backend}\")\n",
        "\n",
        "# Assert performance optimization backend\n",
        "assert performance_backend in ['Pandas', 'Snowflake'], f\"Expected valid backend for performance test, got {performance_backend}\"\n",
        "print(f\"   ✓ ASSERTION PASSED: Multi-level aggregation uses {performance_backend} backend with {optimization_time:.2f}s execution\")\n",
        "\n",
        "print(\"\\n3. Testing enhanced data movement intelligence:\")\n",
        "# Force a scenario that tests intelligent data movement\n",
        "large_sample = sales_df.head(500000)  # 500K rows\n",
        "large_sample_backend = large_sample.get_backend()\n",
        "\n",
        "small_agg = large_sample.groupby('CATEGORY')['TOTAL_AMOUNT'].sum()\n",
        "small_agg_backend = small_agg.get_backend()\n",
        "\n",
        "very_small_result = small_agg.head(3)\n",
        "very_small_backend = very_small_result.get_backend()\n",
        "\n",
        "print(f\"   Large sample backend: {large_sample_backend}\")\n",
        "print(f\"   Aggregated result backend: {small_agg_backend}\")\n",
        "print(f\"   Very small result backend: {very_small_backend}\")\n",
        "\n",
        "# Assert data movement intelligence\n",
        "assert large_sample_backend in ['Pandas', 'Snowflake'], f\"Expected valid backend for large sample, got {large_sample_backend}\"\n",
        "assert small_agg_backend in ['Pandas', 'Snowflake'], f\"Expected valid backend for aggregated result, got {small_agg_backend}\"\n",
        "assert very_small_backend in ['Pandas', 'Snowflake'], f\"Expected valid backend for very small result, got {very_small_backend}\"\n",
        "\n",
        "print(f\"   ✓ ASSERTION PASSED: Data movement intelligence working - backends selected appropriately\")\n",
        "print(f\"   ✓ Large sample (500K): {large_sample_backend}, Aggregated (10 categories): {small_agg_backend}, Very small (3 rows): {very_small_backend}\")\n",
        "\n",
        "print(\"\\n4. Testing dogfood-specific backend explanations:\")\n",
        "try:\n",
        "    pd.explain_switch()\n",
        "    print(\"   ✓ ASSERTION: explain_switch() functionality available and working\")\n",
        "except Exception as e:\n",
        "    print(f\"   ⚠️  explain_switch() not available or error: {e}\")\n",
        "    print(\"   Note: This feature may not be fully implemented in this dogfood version\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. Cleanup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Clean up test tables\n",
        "cleanup = input(\"Do you want to drop the test tables? (y/n): \")\n",
        "\n",
        "if cleanup.lower() == 'y':\n",
        "    session.sql(\"DROP TABLE IF EXISTS sales_transactions\").collect()\n",
        "    print(\"Test tables dropped successfully!\")\n",
        "else:\n",
        "    print(\"Test tables preserved for further testing.\")\n",
        "\n",
        "# Close session\n",
        "session.close()\n",
        "print(\"Session closed.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated the key capabilities of hybrid Snowpark pandas in the **hybrid-pandas-dogfood-1.34.0-python-3.12** environment:\n",
        "\n",
        "### Key Features Tested:\n",
        "1. **Automatic Backend Selection**: The system intelligently chooses between pandas and Snowflake based on data size and operations\n",
        "2. **Seamless Data Movement**: Data moves between backends transparently with progress indicators\n",
        "3. **Performance Optimization**: Large datasets stay in Snowflake for scalable processing, while small results move to pandas for flexibility\n",
        "4. **Manual Control**: When needed, you can override automatic selection and pin backends\n",
        "5. **Real-world Analytics**: Complex analytical workflows benefit from the hybrid approach\n",
        "6. **Dogfood Environment Enhancements**: Version 1.34.0 includes improved switching logic and performance optimizations\n",
        "\n",
        "### Key Takeaways:\n",
        "- Datasets < 10M rows typically use pandas backend\n",
        "- Datasets > 10M rows typically use Snowflake backend  \n",
        "- Filtering and aggregation can trigger backend switches\n",
        "- Memory usage is optimized by keeping large data in Snowflake\n",
        "- Familiar pandas API works across both backends\n",
        "- Enhanced performance in dogfood environment with v1.34.0 optimizations\n",
        "\n",
        "### Best Practices for Dogfood Environment:\n",
        "- Trust the automatic backend selection - it's been optimized in v1.34.0\n",
        "- Use manual control sparingly and with purpose\n",
        "- Chain operations to minimize data movement\n",
        "- Monitor backend switches with `pd.explain_switch()`\n",
        "- Consider data size and operation type when planning workflows\n",
        "- Take advantage of enhanced aggregation performance in this version\n",
        "\n",
        "### Environment-Specific Notes:\n",
        "- This dogfood environment (1.34.0) includes experimental features and optimizations\n",
        "- Performance characteristics may differ from production releases\n",
        "- Backend switching thresholds may be tuned differently for testing purposes\n",
        "- Enhanced debugging and monitoring capabilities are available\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hybrid-pandas-dogfood-1.34.0-python-3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
