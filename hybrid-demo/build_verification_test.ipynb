{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "014b3af4-af71-4b9d-b6f4-c8f990c11d62",
   "metadata": {},
   "source": [
    "# Hybrid Execution Build Verification Test\n",
    "\n",
    "In this demo, we will show how you can develop a robust pandas pipelines at all data scales. You will see how pandas on Snowflake intelligently determines whether to execute queries locally with regular pandas or run directly in Snowflake. This allows you to rapidly iterate with your pandas workflows for testing and development on small datasets, while futureproofing your pipelines when you scale up to production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f420d4-a934-409c-92b9-29d2427ce6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark.modin.plugin\n",
    "import modin.pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as native_pd\n",
    "from time import perf_counter\n",
    "from snowflake.snowpark.session import Session; session = Session.builder.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b532c8-afc6-4b79-9dff-047bc9432b4f",
   "metadata": {},
   "source": [
    "## read_csv loads into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03634f6c-d9fc-44a4-b77e-51df5fe11d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = pd.read_csv('data.csv')\n",
    "fruits.get_backend()\n",
    "assert fruits.get_backend() == \"Pandas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8068f8b4-fd0a-43e1-a68d-87f7d1b30d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6932f458-b4da-4873-8b52-3c85476a921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"s3://sfquickstarts/intro-to-machine-learning-with-snowpark-ml-for-python/diamonds.csv\")\n",
    "assert df.get_backend() == \"Pandas\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662115d3-ad0a-4fea-88ba-321a03ffdf3c",
   "metadata": {},
   "source": [
    "## inline data is in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6a86c-ba58-443e-b48f-80a717eb8204",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_holidays = [\n",
    "    (\"New Year's Day\", \"2025-01-01\"),\n",
    "    (\"Martin Luther King Jr. Day\", \"2025-01-20\"),\n",
    "    (\"Presidents' Day\", \"2025-02-17\"),\n",
    "    (\"Memorial Day\", \"2025-05-26\"),\n",
    "    (\"Juneteenth National Independence Day\", \"2025-06-19\"),\n",
    "    (\"Independence Day\", \"2025-07-04\"),\n",
    "    (\"Labor Day\", \"2025-09-01\"),\n",
    "    (\"Columbus Day\", \"2025-10-13\"),\n",
    "    (\"Veterans Day\", \"2025-11-11\"),\n",
    "    (\"Thanksgiving Day\", \"2025-11-27\"),\n",
    "    (\"Christmas Day\", \"2025-12-25\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_us_holidays = pd.DataFrame(us_holidays, columns=[\"Holiday\", \"Date\"])\n",
    "\n",
    "# Convert Date column to datetime\n",
    "df_us_holidays[\"Date\"] = pd.to_datetime(df_us_holidays[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09f690-e4e6-4aa3-bef5-bc96a389c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_us_holidays.get_backend() == 'Pandas'  # with auto, we should expect this to be local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b150e-086f-4337-bd50-25db48bbeffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns for transformations\n",
    "df_us_holidays[\"Day_of_Week\"] = df_us_holidays[\"Date\"].dt.day_name()\n",
    "df_us_holidays[\"Month\"] = df_us_holidays[\"Date\"].dt.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d304c3-54bc-4ab0-b792-df0e67566819",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c2e23e-d119-4300-9af2-e2f82d4bddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558479c6-11d3-42f0-990a-7914967ab66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Note that without auto-switching, this took 2.5 min\n",
    "for index, row in df_us_holidays.iterrows():\n",
    "    print(f\"{row['Holiday']} falls on {row['Day_of_Week']}, {row['Month']} {row['Date'].day}, {row['Date'].year}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aae1a0-98b3-4c3d-a5f4-4c8d183161a0",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Automatic switching speeds up loops/iterations on small data + inline creation of dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88d966-cd81-400f-b0c5-ceac5731aeba",
   "metadata": {},
   "source": [
    "## Example 2: When data is filtered the choice of engine changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c6d9f6-6168-4f66-8441-5b3ea467e313",
   "metadata": {},
   "source": [
    "Run the following SQL to generate a synthetic dataset with 10M rows of transactions (from 2024-2025 current date)\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE revenue_transactions (\n",
    "    Transaction_ID STRING,\n",
    "    Date DATE,\n",
    "    Revenue FLOAT\n",
    ");\n",
    "\n",
    "SET num_days = (SELECT DATEDIFF(DAY, '2024-01-01', CURRENT_DATE));\n",
    "INSERT INTO revenue_transactions (Transaction_ID, Date, Revenue)\n",
    "SELECT\n",
    "    UUID_STRING() AS Transaction_ID,\n",
    "    DATEADD(DAY, UNIFORM(0, $num_days, RANDOM()), '2024-01-01') AS Date,\n",
    "    UNIFORM(10, 1000, RANDOM()) AS Revenue\n",
    "FROM TABLE(GENERATOR(ROWCOUNT => 10000000));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82055d-74ed-4ec6-b6cc-886753d10cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following to generate a synthetic dataset with 10M rows of transactions (from 2024-2025 current date)\n",
    "session.sql('''\n",
    "CREATE OR REPLACE TABLE revenue_transactions (\n",
    "    Transaction_ID STRING,\n",
    "    Date DATE,\n",
    "    Revenue FLOAT\n",
    ");''').collect()\n",
    "session.sql('''SET num_days = (SELECT DATEDIFF(DAY, '2024-01-01', CURRENT_DATE));''').collect()\n",
    "session.sql('''INSERT INTO revenue_transactions (Transaction_ID, Date, Revenue)\n",
    "SELECT\n",
    "    UUID_STRING() AS Transaction_ID,\n",
    "    DATEADD(DAY, UNIFORM(0, $num_days, RANDOM()), '2024-01-01') AS Date,\n",
    "    UNIFORM(10, 1000, RANDOM()) AS Revenue\n",
    "FROM TABLE(GENERATOR(ROWCOUNT => 10000000));\n",
    "''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2a3cd9-7d32-4242-a358-8dd5aee926dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = pd.read_snowflake(\"REVENUE_TRANSACTIONS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227e992-54cb-4c72-8fc1-69b69f1c9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dataset size is {len(df_transactions)} and the data is located in {df_transactions.get_backend()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa13a851-b9a3-49a7-91c2-7795def50e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dafef83-ee11-4f60-b17b-ce84cc29d854",
   "metadata": {},
   "source": [
    "Perform some operations on 10M rows with Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa6dde-84db-41a8-9070-3d384a576f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions[\"DATE\"] = pd.to_datetime(df_transactions[\"DATE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae5759-2d39-482c-9fbd-bd937c2fbdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_transactions.groupby(\"DATE\").sum()[\"REVENUE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824036de-5ac8-4990-8ab3-cd164c43eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_transactions.get_backend() == \"Snowflake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703d2fa-4d65-42b6-91a3-b56716aa94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733779ec-13d5-4c24-a874-3d21dfc5e759",
   "metadata": {},
   "source": [
    "So far everything has been happening in Snowflake, since we are working with the full dataset (10M rows). \n",
    "Next, we demonstrate what happens when we filter the data down to a smaller dataset below our data size threshold for automatic engine switching. \n",
    "First, let's perform the filtering directly with pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cbfe75-dd9e-4272-b466-fdb0cc0f9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions_filter1 = df_transactions[(df_transactions[\"DATE\"] >= pd.Timestamp.today().date() - pd.Timedelta('7 days')) & (df_transactions[\"DATE\"] < pd.Timestamp.today().date())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87088a-dc85-43f5-8e66-8be46074f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_transactions_filter1.get_backend() == \"Snowflake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e454bdb7-2fff-4c52-ad18-6419e197dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Date range: {df_transactions_filter1['DATE'].min().date()} to {df_transactions_filter1['DATE'].max().date()}. Resulting dataset size: {len(df_transactions_filter1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e416386-07f9-4604-940b-56923d73d463",
   "metadata": {},
   "source": [
    "Now that we have a smaller dataframe, this happens in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcc6f90-f883-49a8-b850-3bf5cadd31a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "df_transactions_filter1 = df_transactions_filter1.groupby(\"DATE\").sum()[\"REVENUE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d151b6-1c48-4d27-a19b-c429abb48f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Waiting on GroupBy switcheroo\n",
    "df_transactions_filter1 = df_transactions_filter1.move_to(\"Pandas\")\n",
    "assert df_transactions_filter1.get_backend()==\"Pandas\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9d9f6c-dc87-42c7-8b3f-e19b36ea5897",
   "metadata": {},
   "source": [
    "We saw what happens when we filter with pandas. Now let's look at what happens if we perform filtering via SQL directly in the `read_snowflake` command, so the dataframe upon creation is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee902a-71d8-412b-83b7-ffbd53b8a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions_filter2 = pd.read_snowflake(\"SELECT * FROM revenue_transactions WHERE Date >= DATEADD( 'days', -7, current_date ) and Date < current_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f024f0ff-b9d7-4c67-9fba-e98b1336300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_transactions_filter2.get_backend()==\"Pandas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf8150-97bb-479f-acb4-d0a4767037db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the result is same as above\n",
    "print(f\"Date range: {df_transactions_filter2['DATE'].min()} to {df_transactions_filter2['DATE'].max()}. Resulting dataset size: {len(df_transactions_filter2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5972d8e-d5d9-4583-8719-415b71f32a8c",
   "metadata": {},
   "source": [
    "Once you are in pandas, you can still continue to perform the same operations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6ea59-9f26-4968-b652-cac35850c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "df_transactions_filter2.groupby(\"DATE\").sum()[\"REVENUE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f187ff8-dd87-4640-803e-0ca7e4867e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_transactions_filter2.get_backend() == 'Pandas'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021bba5b-6f50-48d8-85a4-189668a67fa8",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Automatic switching means that pandas work well for both small and large data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f641537-49d6-4e39-9d28-c9eaf1b1f6af",
   "metadata": {},
   "source": [
    "## Example 3: Combining small and large datasets in the same workflow\n",
    "\n",
    "Soemtimes you are working with multiple dataframes of different sizes and you need to join them together, what happens in this scenario?\n",
    "When two dataframes are joined and the two dataframe are coming from different engine, we automatically determine what is the most optimal way to move the data to minimize the cost of data movement.\n",
    "\n",
    "Continuing with our `df_transactions` and `df_us_holidays` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7514b5-d880-4f32-89a6-65360cfd2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quick recap:\")\n",
    "print(f\"- df_transactions is {len(df_transactions)} rows and the data is located in {df_transactions.get_backend()}.\")\n",
    "print(f\"- df_us_holidays is {len(df_us_holidays)} rows and the data is located in {df_us_holidays.get_backend()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad40f90-255a-49aa-a267-493aa4ca7368",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions[\"DATE\"] = pd.to_datetime(df_transactions[\"DATE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272bf76-8aa6-449a-abb7-cfeff35d2cc2",
   "metadata": {},
   "source": [
    "Since `df_us_holidays` is much smaller than `df_transactions`, we moved `df_us_holidays` to Snowflake where `df_transactions` is, to perform the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f77864-eb02-4899-895d-a5f3fe907650",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(df_us_holidays, df_transactions, left_on=\"Date\", right_on=\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe1e1e-1b79-4053-9aaf-c4a55896ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert combined.get_backend() == \"Snowflake\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2b0dc6-96c5-4e7a-8e00-cc26281bccb0",
   "metadata": {},
   "source": [
    "### ðŸ’¡ When we combine multiple dataframes running in different locations, pandas on Snowflake automatically determines where to move the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab88cfe0-936a-4fa0-ba01-1a7152781d2f",
   "metadata": {},
   "source": [
    "## Example 4: Performing custom `apply` on small dataset\n",
    "\n",
    "apply is known to be slow in Snowpark pandas since it is implemented as UDF/UDTF, which often comes with a fixed startup time.\n",
    "Here, we show an example of how performing `apply` on a small dataset is faster with local pandas. \n",
    "\n",
    "In this example, we want to forecast using last year's transaction data via a custom apply function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a7f0e1-3db6-4a6e-97f3-d28e8d28cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_revenue(df, start_date, end_date):\n",
    "    # Filter data from last year\n",
    "    df_filtered = df[(df[\"DATE\"] >= start_date - pd.Timedelta(days=365)) & (df[\"DATE\"] < start_date)]\n",
    "    # Append future dates to daily_avg for prediction\n",
    "    future_dates = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "    df_future = pd.DataFrame({\"DATE\": future_dates})\n",
    "\n",
    "    # Group by DATE and calculate the mean revenue\n",
    "    daily_avg = df_filtered.groupby(\"DATE\")[\"REVENUE\"].mean().reset_index()\n",
    "    daily_avg[\"DATE\"] = daily_avg[\"DATE\"].astype('datetime64[ns]')\n",
    "    # Merge future dates with predicted revenue, filling missing values\n",
    "    df_forecast = df_future.merge(daily_avg, on=\"DATE\", how=\"left\")\n",
    "    import numpy as np\n",
    "    # Fill missing predicted revenue with overall mean from last year\n",
    "    df_forecast[\"PREDICTED_REVENUE\"] = np.nan\n",
    "    df_forecast[\"PREDICTED_REVENUE\"].fillna(daily_avg[\"REVENUE\"].mean(), inplace=True)\n",
    "    df_forecast[\"PREDICTED_REVENUE\"] = df_forecast[\"PREDICTED_REVENUE\"].astype(\"float\")\n",
    "    return df_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3749373-0500-4df2-8f55-7e5024e7051e",
   "metadata": {},
   "source": [
    "First, let's use the `forecast_revenue` function to get the forecast in the date range, based on last year's revenue numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef702a-60da-48a9-b1e6-90982879afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.Timestamp(\"2025-10-01\")\n",
    "end_date = pd.Timestamp(\"2025-10-31\")\n",
    "df_forecast = forecast_revenue(df_transactions, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276513a5-bcad-4c41-bec0-585ebc5690a4",
   "metadata": {},
   "source": [
    "The resulting dataframe is very small, since it is only the 1-month window we're performing forecast on, so the backend is running on pandas locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5cf071-c8be-4543-b3b9-a23b844b18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Waiting on GroupBy switcheroo\n",
    "#df_forecast = df_forecast.move_to('Pandas')\n",
    "assert df_forecast.get_backend() == 'Snowflake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17271e32-0a0e-47aa-9701-38d35373f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_for_holiday_weekend(row):\n",
    "    # For national holidays, revenue down 5% since stores are closed. For weekends, revenue is up 5% due to increased activity.\n",
    "    if row[\"DATE\"].strftime('%Y-%m-%d') in list(df_us_holidays[\"Date\"].dt.strftime('%Y-%m-%d')): \n",
    "        return row[\"PREDICTED_REVENUE\"] * 0.95\n",
    "    elif row[\"DATE\"].weekday() == 5 or row[\"DATE\"].weekday() == 6: #Saturday/Sundays\n",
    "        return row[\"PREDICTED_REVENUE\"] * 1.05\n",
    "    return row[\"PREDICTED_REVENUE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea50fe2-0633-4b4a-9e51-4cf284b915f1",
   "metadata": {},
   "source": [
    "Now if we run `apply` on this dataframe. It will be running with local pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a06f7-9a55-4d7f-a862-3a73c16b3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust for holidays using the apply function\n",
    "df_forecast[\"PREDICTED_REVENUE\"] = df_forecast.apply(adjust_for_holiday_weekend, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c63b7-885a-4997-a404-ac99bb00aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_forecast.get_backend() == 'Pandas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241afafc-aee4-4c25-8bc9-79c0feba5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Waiting on environment settings\n",
    "from modin.config.envvars import NativePandasMaxRows\n",
    "NativePandasMaxRows.put(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655d2649-8805-4a9d-a7be-a27b95eedcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = pd.DataFrame({'a': [1]*20, 'b': [2]*20}).move_to('Snowflake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c363d9-1c46-447d-8ae0-ffcc478658be",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_small.get_backend() == 'Snowflake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db48d6da-8566-4b12-ae4a-28254c5e7eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df_small.apply(lambda x : x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff18c5-80d0-48bd-a360-e9087711c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_small.get_backend() == 'Snowflake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1425a9-cf16-478b-a461-467c19912729",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df_small.head(5)\n",
    "df_small = df_small.apply(lambda x : x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae22e1-bc54-4dcf-8448-f1c2e45e2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_small.get_backend() == 'Pandas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc492c-6faf-4c2e-9d8a-ab4196397495",
   "metadata": {},
   "outputs": [],
   "source": [
    "NativePandasMaxRows.put(10_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1265c3-ef42-428c-8604-a4e5b66fca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.explain(last=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cbbf9d",
   "metadata": {},
   "source": [
    "# bug fix from bug bash, May 2, 2025: constructing dataframe or series out of snowflake df or series should not cuase data to move (\"Jonathan Shi passing a Series/DF to the DF constructor triggers surprising moves (in)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.DataFrame(list(range(10))).move_to('snowflake')\n",
    "assert input_df.get_backend() == 'Snowflake'\n",
    "assert pd.DataFrame(input_df).get_backend() == 'Snowflake'\n",
    "assert pd.DataFrame({'col0': input_df[0]}).get_backend() == 'Snowflake'\n",
    "assert pd.DataFrame(input_df[0]).get_backend() == 'Snowflake'\n",
    "\n",
    "pandas_df  = pd.DataFrame(list(range(10)))\n",
    "assert pandas_df.get_backend() == 'Pandas'\n",
    "assert pd.DataFrame(pandas_df).get_backend() == 'Pandas'\n",
    "assert pd.DataFrame({'col0': pandas_df[0]}).get_backend() == 'Pandas'\n",
    "assert pd.DataFrame(pandas_df[0]).get_backend() == 'Pandas'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d724dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
