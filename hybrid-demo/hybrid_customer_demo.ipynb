{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "014b3af4-af71-4b9d-b6f4-c8f990c11d62",
   "metadata": {},
   "source": [
    "# pandas on Snowflake - Hybrid Execution Demo\n",
    "\n",
    "In this demo, we will show how you can develop a robust pandas pipelines at all data scales. You will see how pandas on Snowflake intelligently determines whether to execute queries locally with regular pandas or run directly in Snowflake. This allows you to rapidly iterate with your pandas workflows for testing and development on small datasets, while futureproofing your pipelines when you scale up to production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56f420d4-a934-409c-92b9-29d2427ce6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n",
      "Going to open: https://snowbiz.okta.com/app/snowflake/exk8wfsfryJIn4IWZ2p7/sso/saml?SAMLRequest=jVJdb%2BIwEPwrke%2BZ2AlUcBZQUTjUcOVDDVCJN5M41MKxU6%2FTwP36cwiceg%2Bt%2BmatZ3Zmd7Z%2Ff8ql984NCK0GKPAJ8rhKdCrUYYA262mrhzywTKVMasUH6MwB3Q%2F7wHJZ0FFpX9Uzfys5WM81UkDrjwEqjaKagQCqWM6B2oTGo%2FkTDX1CGQA31smhKyUF4bRerS0oxlVV%2BVXb1%2BaAQ0IIJj%2BxQ9WQH%2BiDRPG1RmG01YmWN8rJzfSJRIBJp5ZwCKewuhIfhGpW8JXKvgEBfVyvV63VMl4jb3SbbqwVlDk3MTfvIuGb56fGADgH8WL58rjcxL98ULrKJDvyROdFaV03371wxlMs9UG4HUWTASqOIt1O9S7fx3YG80ALwbbzeZZGvflsPMukKNrd9ls4%2Bb3Yn6JegrztLdGwTjQCKHmk6hytK5HwrkU6rSBYE0Lbd5QQv9vt7pA3cTkKxeyFeTNbW9yLP74%2BWnYxx4oC%2F%2FON%2BenYqzLIzHkWqU70sguLLgbQuI4JNZdCLwbM8Lvz9%2FFH1vXYFm7%2F0WSlpUjO3lSbnNnP4wn84FIRaSu7QCnPmZCjNDUcwMUkpa7GhjPrbtqakiM8bFT%2Fv%2BrhXw%3D%3D&RelayState=ver%3A1-hint%3A2050423465438-ETMsDgAAAZYiRXopABRBRVMvQ0JDL1BLQ1M1UGFkZGluZwEAABAAEIAIXvJaNiER3rtpPDuYxC8AAACQbZsc0STpWMZ%2BExVM%2BmiRQAlc9DgoHZNhcAO5HBoX3c%2B5sYHd%2BLd63GHwoQPKPIVmID%2F%2FzgGCJf79s08NpVFuZFqPUo4nieJ0R9GS4cSGsK04w0eOykHHiH766RazFCTKbmsPc9UnuRNOhxzr5Whur9m72X2u%2BKV0VgypJIF%2BZktGKKOeJfU6iSAfB7ikMPTLABTHtqwG3u7fj41HOfvg10H7prkvBA%3D%3D to authenticate...\n"
     ]
    }
   ],
   "source": [
    "import snowflake.snowpark.modin.plugin\n",
    "import modin.pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as native_pd\n",
    "from time import perf_counter\n",
    "from snowflake.snowpark.session import Session; session = Session.builder.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6988af-98a1-4d59-92c6-e13c689c603b",
   "metadata": {},
   "source": [
    "## Generate Data Tables\n",
    "First let's generate synthetic data. Note that this will take a while and only needs to be run once. \n",
    "```python\n",
    "def generate_synthetic_table(N,name): # Run the following to generate a synthetic dataset with X rows of transactions (from 2024-2025 current date)\n",
    "    session.sql(f'''\n",
    "    CREATE OR REPLACE TABLE revenue_transactions_{name} (\n",
    "        Transaction_ID STRING,\n",
    "        Date DATE,\n",
    "        Revenue FLOAT\n",
    "    );''').collect()\n",
    "    session.sql('''SET num_days = (SELECT DATEDIFF(DAY, '2024-01-01', CURRENT_DATE));''').collect()\n",
    "    session.sql(f'''INSERT INTO revenue_transactions_{name} (Transaction_ID, Date, Revenue)\n",
    "    SELECT\n",
    "        UUID_STRING() AS Transaction_ID,\n",
    "        DATEADD(DAY, UNIFORM(0, $num_days, RANDOM()), '2024-01-01') AS Date,\n",
    "        UNIFORM(10, 1000, RANDOM()) AS Revenue\n",
    "    FROM TABLE(GENERATOR(ROWCOUNT => {N}));\n",
    "    ''').collect()\n",
    "\n",
    "generate_synthetic_table(10000000, \"10M\")\n",
    "generate_synthetic_table(100000000, \"100M\")\n",
    "generate_synthetic_table(1000000000, \"1B\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea967fb-edae-4123-b5ee-15d874b66615",
   "metadata": {},
   "source": [
    "## Reading Data From Snowflake\n",
    "\n",
    "There are two common approaches to reading the data to vanilla pandas. \n",
    "\n",
    "1) Create a [Snowpark DataFrame](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes#return-the-contents-of-a-dataframe-as-a-pandas-dataframe) and calling [`to_pandas`](https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.DataFrame.to_pandas) to export results into a pandas DataFrame\n",
    "```python\n",
    "snowpark_df = session.table(\"REVENUE_TRANSACTIONS_10M\")\n",
    "native_pd_df = snowpark_df.to_pandas()\n",
    "```\n",
    "\n",
    "2) Use the [Snowflake Connector for Python](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-pandas) to query and export results from Snowflake into a pandas DataFrame using [`fetch_pandas_all`](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#fetch_pandas_all)\n",
    "\n",
    "```python\n",
    "# Create a cursor object\n",
    "cur = session.connection.cursor()\n",
    "# Execute a statement that will generate a result set\n",
    "cur.execute(\"select * from REVENUE_TRANSACTIONS_10M\")\n",
    "# Fetch all the rows in a cursor and load them into a pandas DataFrame\n",
    "native_pd_df = cur.fetch_pandas_all()\n",
    "```\n",
    "\n",
    "We will use the first approach below and measure the time these operations take. (Note: This may take several minutes!)\n",
    "\n",
    "Now let's try to load in the 10M row table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "372ba36e-c5a0-4356-bb6f-2f401cd722f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 10M rows to pandas dataframes takes 25.496354874921963 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "table = session.table(\"REVENUE_TRANSACTIONS_10M\")\n",
    "pandas_df_10M = table.to_pandas()\n",
    "end_time = perf_counter()\n",
    "time_10M = end_time-start_time\n",
    "print(f\"Loading 10M rows to pandas dataframes takes {time_10M} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f3c50c-afe8-4fdd-88cb-19e7a7f2dc0b",
   "metadata": {},
   "source": [
    "Now let's try to load in the 100M row table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e130400-461a-4617-a555-0a2c34df1ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100M rows to pandas dataframes takes 292.1991420829436 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "table = session.table(\"REVENUE_TRANSACTIONS_100M\")\n",
    "pandas_df_100M = table.to_pandas()\n",
    "end_time = perf_counter()\n",
    "time_100M = end_time-start_time\n",
    "print(f\"Loading 100M rows to pandas dataframes takes {time_100M} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3dd639-105f-4c8e-a5e5-333c13e8ad86",
   "metadata": {},
   "source": [
    "Now if we try doing `to_pandas` on the 1 billion row table, we will get an out of memory error that looks something like this\n",
    "```python\n",
    "start_time = time.time()\n",
    "table = session.table(\"REVENUE_TRANSACTIONS_1B\")\n",
    "pandas_df = table.to_pandas()\n",
    "end_time = time.time()\n",
    "time_1B = end_time-start_time\n",
    "```\n",
    "<img src=\"OOM.png\" alt=\"out of memory\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1c449-6307-4f49-8c8c-c78184123356",
   "metadata": {},
   "source": [
    "Now let's try this with Snowpark pandas. We can read the table directly using Snowpark pandas's `read_snowflake` command, without pulling the full dataset into memory. Snowpark pandas pushes down the computation to run on Snowflake, so that we can operate on the data directly without pulling the dataset into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a5d1bb-74c7-4a8d-b38c-0c42b9635d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 10M rows using Snowpark pandas takes 2.2936937080230564 seconds\n"
     ]
    }
   ],
   "source": [
    "# Read data into a Snowpark pandas df \n",
    "start = perf_counter()\n",
    "snowpandas_df_10M = pd.read_snowflake(\"REVENUE_TRANSACTIONS_10M\")\n",
    "end = perf_counter()\n",
    "snowtime_10M = end - start\n",
    "print(f\"Loading 10M rows using Snowpark pandas takes {snowtime_10M} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a5cdc44-4baa-438b-b3a2-4d4951585f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100M rows using Snowpark pandas takes 1.9097921670181677 seconds\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "snowpandas_df_100M = pd.read_snowflake(\"REVENUE_TRANSACTIONS_100M\")\n",
    "end = perf_counter()\n",
    "snowtime_100M = end - start\n",
    "print(f\"Loading 100M rows using Snowpark pandas takes {snowtime_100M} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87f863ad-33fc-46cf-9b60-b99b6e5a0c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1B rows using Snowpark pandas takes 3.130773166078143 seconds\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "snowpandas_df_1B = pd.read_snowflake(\"REVENUE_TRANSACTIONS_1B\")\n",
    "end = perf_counter()\n",
    "snowtime_1B = end - start\n",
    "print(f\"Loading 1B rows using Snowpark pandas takes {snowtime_1B} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2dd070-05ff-4f33-947b-ff3a8e27fc05",
   "metadata": {},
   "source": [
    "As you can see, calling `read_snowflake` on any sized data takes no more than a few seconds, even for the 1 billion row dataset where pandas runs out of memory. We can also performs some operations using these dataframes, which finishes in a few seconds. \n",
    "\n",
    "In contrast, running this `pandas_df_10M.groupby(\"DATE\").sum()[\"REVENUE\"]` in pandas just hangs ... and forcing us to kill the Python kernel after some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "084b7c6e-337c-46dd-96cd-d3ba81e3321e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE\n",
      "2024-01-01    10636116.0\n",
      "2024-01-02    10871886.0\n",
      "2024-01-03    10821764.0\n",
      "2024-01-04    10912870.0\n",
      "2024-01-05    10809808.0\n",
      "                 ...    \n",
      "2025-04-06    10776615.0\n",
      "2025-04-07    10938497.0\n",
      "2025-04-08    10737509.0\n",
      "2025-04-09    10777891.0\n",
      "2025-04-10    10715432.0\n",
      "Name: REVENUE, Length: 466, dtype: float64\n",
      "Performing groupby operation on 100M rows using Snowpark pandas takes 2.017078666947782 seconds\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "snowpandas_groupby = snowpandas_df_10M.groupby(\"DATE\").sum()[\"REVENUE\"]\n",
    "print(snowpandas_groupby)\n",
    "end = perf_counter()\n",
    "snowpandas_time_10M = end - start\n",
    "print(f\"Performing groupby operation on 100M rows using Snowpark pandas takes {snowpandas_time_10M} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcd8f3e1-6a62-439d-be18-53fad63d5155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE\n",
      "2024-01-01    108024176.0\n",
      "2024-01-02    108655406.0\n",
      "2024-01-03    108026482.0\n",
      "2024-01-04    108137523.0\n",
      "2024-01-05    108316254.0\n",
      "                 ...     \n",
      "2025-04-06    108543057.0\n",
      "2025-04-07    108243461.0\n",
      "2025-04-08    108517153.0\n",
      "2025-04-09    108425654.0\n",
      "2025-04-10    108194452.0\n",
      "Name: REVENUE, Length: 466, dtype: float64\n",
      "Performing groupby operation on 100M rows using Snowpark pandas takes 1.8401533330325037 seconds\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "snowpandas_groupby = snowpandas_df_100M.groupby(\"DATE\").sum()[\"REVENUE\"]\n",
    "print(snowpandas_groupby)\n",
    "end = perf_counter()\n",
    "snowpandas_time_100M = end - start\n",
    "print(f\"Performing groupby operation on 100M rows using Snowpark pandas takes {snowpandas_time_100M} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef2c87b6-6a68-41d1-9e2c-798e9dd42503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE\n",
      "2024-01-01    1.085032e+09\n",
      "2024-01-02    1.083547e+09\n",
      "2024-01-03    1.085693e+09\n",
      "2024-01-04    1.083782e+09\n",
      "2024-01-05    1.084499e+09\n",
      "                  ...     \n",
      "2025-04-06    1.083556e+09\n",
      "2025-04-07    1.083573e+09\n",
      "2025-04-08    1.084559e+09\n",
      "2025-04-09    1.083182e+09\n",
      "2025-04-10    1.083395e+09\n",
      "Name: REVENUE, Length: 466, dtype: float64\n",
      "Performing groupby operation on 1B rows using Snowpark pandas takes 5.48939924989827 seconds\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "snowpandas_groupby = snowpandas_df_1B.groupby(\"DATE\").sum()[\"REVENUE\"]\n",
    "print(snowpandas_groupby)\n",
    "end = perf_counter()\n",
    "snowpandas_time_1B = end - start\n",
    "print(f\"Performing groupby operation on 1B rows using Snowpark pandas takes {snowpandas_time_1B} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21b06ce-a985-47a7-9760-ce47c47d6fcf",
   "metadata": {},
   "source": [
    "In summary, with Snowpark pandas we are able to easily work with datasets with 1 billion rows and run the operations in a matter a few seconds, while we run into out of memory errors with regular pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42153ca3-38ad-414e-8249-48f2ac4e14a8",
   "metadata": {},
   "source": [
    "## But life isn't always about big data ... \n",
    "\n",
    "Oftentimes you need to start with small data for development and testing. Other times you may be working on projects with data across different scales or problems where your data scale evolves throughout your workflow.\n",
    "\n",
    "pandas is highly optimized for small data scales. This means that sometimes, pushing the computation down to Snowflake may not always be the best choice in terms of performance. In general, the performance improvement from Snowpark starts to show up when we reach the millions-row (10s MB) range.\n",
    "\n",
    "We recently introduced Hybrid Execution for Snowpark pandas to support working with data at all scale, by automatically selecting what engine is best suited for your workload. This means that you know longer have to toggle between what runs well in vanilla pandas vs what runs well in Snowpark, and can focus on developing your Python code, we figure out where is the best place to run your code. \n",
    "\n",
    "Let's take a look at some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662115d3-ad0a-4fea-88ba-321a03ffdf3c",
   "metadata": {},
   "source": [
    "## Example 1: Working with small/inline-created dataframe is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ad6a86c-ba58-443e-b48f-80a717eb8204",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_holidays = [\n",
    "    (\"New Year's Day\", \"2025-01-01\"),\n",
    "    (\"Martin Luther King Jr. Day\", \"2025-01-20\"),\n",
    "    (\"Presidents' Day\", \"2025-02-17\"),\n",
    "    (\"Memorial Day\", \"2025-05-26\"),\n",
    "    (\"Juneteenth National Independence Day\", \"2025-06-19\"),\n",
    "    (\"Independence Day\", \"2025-07-04\"),\n",
    "    (\"Labor Day\", \"2025-09-01\"),\n",
    "    (\"Columbus Day\", \"2025-10-13\"),\n",
    "    (\"Veterans Day\", \"2025-11-11\"),\n",
    "    (\"Thanksgiving Day\", \"2025-11-27\"),\n",
    "    (\"Christmas Day\", \"2025-12-25\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_us_holidays = pd.DataFrame(us_holidays, columns=[\"Holiday\", \"Date\"])\n",
    "\n",
    "# Convert Date column to datetime\n",
    "df_us_holidays[\"Date\"] = pd.to_datetime(df_us_holidays[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de09f690-e4e6-4aa3-bef5-bc96a389c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_us_holidays.get_backend() == 'Pandas'  # with auto, we should expect this to be local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f05b150e-086f-4337-bd50-25db48bbeffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns for transformations\n",
    "df_us_holidays[\"Day_of_Week\"] = df_us_holidays[\"Date\"].dt.day_name()\n",
    "df_us_holidays[\"Month\"] = df_us_holidays[\"Date\"].dt.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4d304c3-54bc-4ab0-b792-df0e67566819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Holiday</th>\n",
       "      <th>Date</th>\n",
       "      <th>Day_of_Week</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Martin Luther King Jr. Day</td>\n",
       "      <td>2025-01-20</td>\n",
       "      <td>Monday</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Presidents' Day</td>\n",
       "      <td>2025-02-17</td>\n",
       "      <td>Monday</td>\n",
       "      <td>February</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Memorial Day</td>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>Monday</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Juneteenth National Independence Day</td>\n",
       "      <td>2025-06-19</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>June</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Independence Day</td>\n",
       "      <td>2025-07-04</td>\n",
       "      <td>Friday</td>\n",
       "      <td>July</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Labor Day</td>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>Monday</td>\n",
       "      <td>September</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Columbus Day</td>\n",
       "      <td>2025-10-13</td>\n",
       "      <td>Monday</td>\n",
       "      <td>October</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Veterans Day</td>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>November</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Thanksgiving Day</td>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>November</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Christmas Day</td>\n",
       "      <td>2025-12-25</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Holiday       Date Day_of_Week      Month\n",
       "0                         New Year's Day 2025-01-01   Wednesday    January\n",
       "1             Martin Luther King Jr. Day 2025-01-20      Monday    January\n",
       "2                        Presidents' Day 2025-02-17      Monday   February\n",
       "3                           Memorial Day 2025-05-26      Monday        May\n",
       "4   Juneteenth National Independence Day 2025-06-19    Thursday       June\n",
       "5                       Independence Day 2025-07-04      Friday       July\n",
       "6                              Labor Day 2025-09-01      Monday  September\n",
       "7                           Columbus Day 2025-10-13      Monday    October\n",
       "8                           Veterans Day 2025-11-11     Tuesday   November\n",
       "9                       Thanksgiving Day 2025-11-27    Thursday   November\n",
       "10                         Christmas Day 2025-12-25    Thursday   December"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_us_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "558479c6-11d3-42f0-990a-7914967ab66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Year's Day falls on Wednesday, January 1, 2025.\n",
      "Martin Luther King Jr. Day falls on Monday, January 20, 2025.\n",
      "Presidents' Day falls on Monday, February 17, 2025.\n",
      "Memorial Day falls on Monday, May 26, 2025.\n",
      "Juneteenth National Independence Day falls on Thursday, June 19, 2025.\n",
      "Independence Day falls on Friday, July 4, 2025.\n",
      "Labor Day falls on Monday, September 1, 2025.\n",
      "Columbus Day falls on Monday, October 13, 2025.\n",
      "Veterans Day falls on Tuesday, November 11, 2025.\n",
      "Thanksgiving Day falls on Thursday, November 27, 2025.\n",
      "Christmas Day falls on Thursday, December 25, 2025.\n",
      "CPU times: user 78.7 ms, sys: 1.92 ms, total: 80.6 ms\n",
      "Wall time: 82 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Note that without auto-switching, this took 2.5 min\n",
    "for index, row in df_us_holidays.iterrows():\n",
    "    print(f\"{row['Holiday']} falls on {row['Day_of_Week']}, {row['Month']} {row['Date'].day}, {row['Date'].year}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aae1a0-98b3-4c3d-a5f4-4c8d183161a0",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Automatic switching speeds up loops/iterations on small data + inline creation of dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88d966-cd81-400f-b0c5-ceac5731aeba",
   "metadata": {},
   "source": [
    "## Example 2: When data is filtered the choice of engine changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c6d9f6-6168-4f66-8441-5b3ea467e313",
   "metadata": {},
   "source": [
    "Run the following SQL to generate a synthetic dataset with 10M rows of transactions (from 2024-2025 current date)\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE revenue_transactions (\n",
    "    Transaction_ID STRING,\n",
    "    Date DATE,\n",
    "    Revenue FLOAT\n",
    ");\n",
    "\n",
    "SET num_days = (SELECT DATEDIFF(DAY, '2024-01-01', CURRENT_DATE));\n",
    "INSERT INTO revenue_transactions (Transaction_ID, Date, Revenue)\n",
    "SELECT\n",
    "    UUID_STRING() AS Transaction_ID,\n",
    "    DATEADD(DAY, UNIFORM(0, $num_days, RANDOM()), '2024-01-01') AS Date,\n",
    "    UNIFORM(10, 1000, RANDOM()) AS Revenue\n",
    "FROM TABLE(GENERATOR(ROWCOUNT => 10000000));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da82055d-74ed-4ec6-b6cc-886753d10cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(number of rows inserted=10000000)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the following to generate a synthetic dataset with 10M rows of transactions (from 2024-2025 current date)\n",
    "session.sql('''\n",
    "CREATE OR REPLACE TABLE revenue_transactions (\n",
    "    Transaction_ID STRING,\n",
    "    Date DATE,\n",
    "    Revenue FLOAT\n",
    ");''').collect()\n",
    "session.sql('''SET num_days = (SELECT DATEDIFF(DAY, '2024-01-01', CURRENT_DATE));''').collect()\n",
    "session.sql('''INSERT INTO revenue_transactions (Transaction_ID, Date, Revenue)\n",
    "SELECT\n",
    "    UUID_STRING() AS Transaction_ID,\n",
    "    DATEADD(DAY, UNIFORM(0, $num_days, RANDOM()), '2024-01-01') AS Date,\n",
    "    UNIFORM(10, 1000, RANDOM()) AS Revenue\n",
    "FROM TABLE(GENERATOR(ROWCOUNT => 10000000));\n",
    "''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af2a3cd9-7d32-4242-a358-8dd5aee926dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = pd.read_snowflake(\"REVENUE_TRANSACTIONS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0227e992-54cb-4c72-8fc1-69b69f1c9570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset size is 10000000 and the data is located in Snowflake.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset size is {len(df_transactions)} and the data is located in {df_transactions.get_backend()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dafef83-ee11-4f60-b17b-ce84cc29d854",
   "metadata": {},
   "source": [
    "Perform some operations on 10M rows with Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9aa6dde-84db-41a8-9070-3d384a576f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions[\"DATE\"] = pd.to_datetime(df_transactions[\"DATE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2ae5759-2d39-482c-9fbd-bd937c2fbdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.2 ms, sys: 10.2 ms, total: 34.5 ms\n",
      "Wall time: 202 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DATE\n",
       "2024-01-01    10982720.0\n",
       "2024-01-02    10646998.0\n",
       "2024-01-03    10962135.0\n",
       "2024-01-04    10812139.0\n",
       "2024-01-05    10811396.0\n",
       "                 ...    \n",
       "2025-04-07    10750499.0\n",
       "2025-04-08    10781859.0\n",
       "2025-04-09    10743897.0\n",
       "2025-04-10    10859036.0\n",
       "2025-04-11    10856796.0\n",
       "Freq: None, Name: REVENUE, Length: 467, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_transactions.groupby(\"DATE\").sum()[\"REVENUE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "824036de-5ac8-4990-8ab3-cd164c43eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_transactions.get_backend() == \"Snowflake\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733779ec-13d5-4c24-a874-3d21dfc5e759",
   "metadata": {},
   "source": [
    "So far everything has been happening in Snowflake, since we are working with the full dataset (10M rows). \n",
    "Next, we demonstrate what happens when we filter the data down to a smaller dataset below our data size threshold for automatic engine switching. \n",
    "First, let's perform the filtering directly with pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23cbfe75-dd9e-4272-b466-fdb0cc0f9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions_filter1 = df_transactions[(df_transactions[\"DATE\"] >= pd.Timestamp.today().date() - pd.Timedelta('7 days')) & (df_transactions[\"DATE\"] < pd.Timestamp.today().date())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b717a5-339d-4d99-a41c-6095504b0486",
   "metadata": {},
   "source": [
    "In this case, since the data is already in Snowflake, it stays in Snowflake even after the filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc87088a-dc85-43f5-8e66-8be46074f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_transactions_filter1.get_backend() == \"Snowflake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e454bdb7-2fff-4c52-ad18-6419e197dbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The current operation leads to materialization and can be slow if the data is large!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2025-04-03 to 2025-04-09. Resulting dataset size: 149559\n"
     ]
    }
   ],
   "source": [
    "print(f\"Date range: {df_transactions_filter1['DATE'].min().date()} to {df_transactions_filter1['DATE'].max().date()}. Resulting dataset size: {len(df_transactions_filter1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e416386-07f9-4604-940b-56923d73d463",
   "metadata": {},
   "source": [
    "Now that we have a smaller dataframe, this happens in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abcc6f90-f883-49a8-b850-3bf5cadd31a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 Î¼s, sys: 0 ns, total: 4 Î¼s\n",
      "Wall time: 13.1 Î¼s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e8142cadaf4c6d971ab6883c068492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transferring data from Snowflake to Pandas ...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DATE\n",
       "2025-04-03    10795147.0\n",
       "2025-04-04    10883893.0\n",
       "2025-04-05    10862456.0\n",
       "2025-04-06    10848465.0\n",
       "2025-04-07    10750499.0\n",
       "2025-04-08    10781859.0\n",
       "2025-04-09    10743897.0\n",
       "Freq: None, Name: REVENUE, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "df_transactions_filter1.groupby(\"DATE\").sum()[\"REVENUE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9d9f6c-dc87-42c7-8b3f-e19b36ea5897",
   "metadata": {},
   "source": [
    "We saw what happens when we filter with pandas. Now let's look at what happens if we perform filtering via SQL directly in the `read_snowflake` command, so the dataframe upon creation is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cee902a-71d8-412b-83b7-ffbd53b8a4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b934f33ff54516a66a9617c503015d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transferring data from Snowflake to Pandas ...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_transactions_filter2 = pd.read_snowflake(\"SELECT * FROM revenue_transactions WHERE Date >= DATEADD( 'days', -7, current_date ) and Date < current_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f024f0ff-b9d7-4c67-9fba-e98b1336300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_transactions_filter2.get_backend()==\"Pandas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eaaf8150-97bb-479f-acb4-d0a4767037db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2025-04-04 to 2025-04-10. Resulting dataset size: 149739\n"
     ]
    }
   ],
   "source": [
    "# Verify the result is same as above\n",
    "print(f\"Date range: {df_transactions_filter2['DATE'].min()} to {df_transactions_filter2['DATE'].max()}. Resulting dataset size: {len(df_transactions_filter2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5972d8e-d5d9-4583-8719-415b71f32a8c",
   "metadata": {},
   "source": [
    "Once you are in pandas, you can still continue to perform the same operations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dd6ea59-9f26-4968-b652-cac35850c123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 1 Î¼s, total: 2 Î¼s\n",
      "Wall time: 4.77 Î¼s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DATE\n",
       "2025-04-04    10883893.0\n",
       "2025-04-05    10862456.0\n",
       "2025-04-06    10848465.0\n",
       "2025-04-07    10750499.0\n",
       "2025-04-08    10781859.0\n",
       "2025-04-09    10743897.0\n",
       "2025-04-10    10859036.0\n",
       "Name: REVENUE, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "df_transactions_filter2.groupby(\"DATE\").sum()[\"REVENUE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021bba5b-6f50-48d8-85a4-189668a67fa8",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Automatic switching means that pandas work well for both small and large data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f641537-49d6-4e39-9d28-c9eaf1b1f6af",
   "metadata": {},
   "source": [
    "## Example 3: Combining small and large datasets in the same workflow\n",
    "\n",
    "Soemtimes you are working with multiple dataframes of different sizes and you need to join them together, what happens in this scenario?\n",
    "When two dataframes are joined and the two dataframe are coming from different engine, we automatically determine what is the most optimal way to move the data to minimize the cost of data movement.\n",
    "\n",
    "Continuing with our `df_transactions` and `df_us_holidays` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a7514b5-d880-4f32-89a6-65360cfd2d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick recap:\n",
      "- df_transactions is 10000000 rows and the data is located in Snowflake.\n",
      "- df_us_holidays is 11 rows and the data is located in Pandas.\n"
     ]
    }
   ],
   "source": [
    "print(\"Quick recap:\")\n",
    "print(f\"- df_transactions is {len(df_transactions)} rows and the data is located in {df_transactions.get_backend()}.\")\n",
    "print(f\"- df_us_holidays is {len(df_us_holidays)} rows and the data is located in {df_us_holidays.get_backend()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ad40f90-255a-49aa-a267-493aa4ca7368",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions[\"DATE\"] = pd.to_datetime(df_transactions[\"DATE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272bf76-8aa6-449a-abb7-cfeff35d2cc2",
   "metadata": {},
   "source": [
    "Since `df_us_holidays` is much smaller than `df_transactions`, we moved `df_us_holidays` to Snowflake where `df_transactions` is, to perform the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88f77864-eb02-4899-895d-a5f3fe907650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b4fcca27f043cc86d71ca8ad91ac68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transferring data from Pandas to Snowflake ...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined = pd.merge(df_us_holidays, df_transactions, left_on=\"Date\", right_on=\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fbe1e1e-1b79-4053-9aaf-c4a55896ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert combined.get_backend() == \"Snowflake\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2b0dc6-96c5-4e7a-8e00-cc26281bccb0",
   "metadata": {},
   "source": [
    "### ðŸ’¡ When we combine multiple dataframes running in different locations, pandas on Snowflake automatically determines where to move the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab88cfe0-936a-4fa0-ba01-1a7152781d2f",
   "metadata": {},
   "source": [
    "## Example 4: Performing custom `apply` on small dataset\n",
    "\n",
    "apply is known to be slow in Snowpark pandas since it is implemented as UDF/UDTF, which often comes with a fixed startup time.\n",
    "Here, we show an example of how performing `apply` on a small dataset is faster with local pandas. \n",
    "\n",
    "In this example, we want to forecast using last year's transaction data via a custom apply function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11a7f0e1-3db6-4a6e-97f3-d28e8d28cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_revenue(df, start_date, end_date):\n",
    "    # Filter data from last year\n",
    "    df_filtered = df[(df[\"DATE\"] >= start_date - pd.Timedelta(days=365)) & (df[\"DATE\"] < start_date)]\n",
    "    \n",
    "    # Append future dates to daily_avg for prediction\n",
    "    future_dates = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "    df_future = pd.DataFrame({\"DATE\": future_dates})\n",
    "\n",
    "    # Group by DATE and calculate the mean revenue\n",
    "    daily_avg = df_filtered.groupby(\"DATE\")[\"REVENUE\"].mean().reset_index()\n",
    "    daily_avg[\"DATE\"] = daily_avg[\"DATE\"].astype('datetime64[ns]')\n",
    "    # Merge future dates with predicted revenue, filling missing values\n",
    "    df_forecast = df_future.merge(daily_avg, on=\"DATE\", how=\"left\")\n",
    "    import numpy as np\n",
    "    # Fill missing predicted revenue with overall mean from last year\n",
    "    df_forecast[\"PREDICTED_REVENUE\"] = np.nan\n",
    "    df_forecast[\"PREDICTED_REVENUE\"].fillna(daily_avg[\"REVENUE\"].mean(), inplace=True)\n",
    "    df_forecast[\"PREDICTED_REVENUE\"] = df_forecast[\"PREDICTED_REVENUE\"].astype(\"float\")\n",
    "    return df_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3749373-0500-4df2-8f55-7e5024e7051e",
   "metadata": {},
   "source": [
    "First, let's use the `forecast_revenue` function to get the forecast in the date range, based on last year's revenue numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33ef702a-60da-48a9-b1e6-90982879afcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ba8aae54b24f2c8c6759ab72d09cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transferring data from Snowflake to Pandas ...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_date = pd.Timestamp(\"2025-10-01\")\n",
    "end_date = pd.Timestamp(\"2025-10-31\")\n",
    "df_forecast = forecast_revenue(df_transactions, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276513a5-bcad-4c41-bec0-585ebc5690a4",
   "metadata": {},
   "source": [
    "The resulting dataframe is very small, since it is only the 1-month window we're performing forecast on, so the backend is running on pandas locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc5cf071-c8be-4543-b3b9-a23b844b18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_forecast.get_backend() == 'Pandas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17271e32-0a0e-47aa-9701-38d35373f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_for_holiday_weekend(row):\n",
    "    # For national holidays, revenue down 5% since stores are closed. For weekends, revenue is up 5% due to increased activity.\n",
    "    if row[\"DATE\"].strftime('%Y-%m-%d') in list(df_us_holidays[\"Date\"].dt.strftime('%Y-%m-%d')): \n",
    "        return row[\"PREDICTED_REVENUE\"] * 0.95\n",
    "    elif row[\"DATE\"].weekday() == 5 or row[\"DATE\"].weekday() == 6: #Saturday/Sundays\n",
    "        return row[\"PREDICTED_REVENUE\"] * 1.05\n",
    "    return row[\"PREDICTED_REVENUE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea50fe2-0633-4b4a-9e51-4cf284b915f1",
   "metadata": {},
   "source": [
    "Now if we run `apply` on this dataframe. It will be running with local pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b17a06f7-9a55-4d7f-a862-3a73c16b3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust for holidays using the apply function\n",
    "df_forecast[\"PREDICTED_REVENUE\"] = df_forecast.apply(adjust_for_holiday_weekend, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c83c63b7-885a-4997-a404-ac99bb00aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_forecast.get_backend() == 'Pandas'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d72d1-f718-4037-bada-df87a51eff74",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Apply on small dataset is much faster with automatic switching running with pandas locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1333db02-0df4-4cae-b8cf-bc09af58dbff",
   "metadata": {},
   "source": [
    "## Example 5: Seamless continuation with downstream code upon backend change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5c5232-827e-4b88-a455-be2d0cead168",
   "metadata": {},
   "source": [
    "Finally, let's plot the resulting forecast as a visualization. Note that the data type of the dataframe stays the same (`modin.pandas.dataframe.DataFrame`)independent of the choice of backend. This ensures compatibility and interoperability with any downstream code when the backend changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3337f57a-8b5d-42e5-9909-ebb0daf62c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Altair takes in <class 'modin.pandas.dataframe.DataFrame'> with Pandas as backend, since we implement the dataframe interchange protocol\n"
     ]
    }
   ],
   "source": [
    "print(f\"Altair takes in {type(df_forecast)} with {df_forecast.get_backend()} as backend, since we implement the dataframe interchange protocol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15a869dc-cf91-4728-918e-f5dfc6713881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Starting with pandas version 3.0 all arguments of to_dict except for the argument 'orient' will be keyword-only.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-14cfd3040077431eb24105cd08cf4482.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-14cfd3040077431eb24105cd08cf4482.vega-embed details,\n",
       "  #altair-viz-14cfd3040077431eb24105cd08cf4482.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-14cfd3040077431eb24105cd08cf4482\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-14cfd3040077431eb24105cd08cf4482\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-14cfd3040077431eb24105cd08cf4482\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-2ed8ce964988ea724ec65283bb9b8648\"}, \"mark\": {\"type\": \"line\", \"color\": \"blue\"}, \"encoding\": {\"tooltip\": [{\"field\": \"DATE\", \"type\": \"temporal\"}, {\"field\": \"PREDICTED_REVENUE\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"DATE\", \"timeUnit\": \"monthdate\", \"type\": \"temporal\"}, \"y\": {\"field\": \"PREDICTED_REVENUE\", \"scale\": {\"domain\": [400, 600]}, \"type\": \"quantitative\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-2ed8ce964988ea724ec65283bb9b8648\": [{\"DATE\": \"2025-10-01T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-02T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-03T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-04T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 530.4670767202472}, {\"DATE\": \"2025-10-05T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 530.4670767202472}, {\"DATE\": \"2025-10-06T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-07T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-08T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-09T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-10T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-11T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 530.4670767202472}, {\"DATE\": \"2025-10-12T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 530.4670767202472}, {\"DATE\": \"2025-10-13T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 479.9464027468902}, {\"DATE\": \"2025-10-14T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-15T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-16T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-17T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-18T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 530.4670767202472}, {\"DATE\": \"2025-10-19T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 530.4670767202472}, {\"DATE\": \"2025-10-20T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-21T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-22T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-23T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-24T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-25T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 530.4670767202472}, {\"DATE\": \"2025-10-26T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 530.4670767202472}, {\"DATE\": \"2025-10-27T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-28T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-29T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-30T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}, {\"DATE\": \"2025-10-31T00:00:00.000000\", \"REVENUE\": NaN, \"PREDICTED_REVENUE\": 505.20673973356867}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "chart_predicted = alt.Chart(df_forecast).mark_line(color='blue').encode(\n",
    "    x='monthdate(DATE):T',\n",
    "    y=alt.Y('PREDICTED_REVENUE:Q',scale=alt.Scale(domain=[400, 600])),\n",
    "    tooltip=['DATE', 'PREDICTED_REVENUE']\n",
    ")\n",
    "chart_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57f6d2c8-44ab-4ced-9a34-bc2b56277423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4452e2c1ed58454f8aa9c1bee26bd9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transferring data from Snowflake to Pandas ...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_transactions_filtered = df_transactions[\n",
    "    (df_transactions[\"DATE\"] >= start_date - pd.Timedelta(days=365)) &\n",
    "    (df_transactions[\"DATE\"] < end_date - pd.Timedelta(days=365))\n",
    "]\n",
    "df_transactions_filtered_groupby = df_transactions_filtered.groupby(\"DATE\")[\"REVENUE\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07066876-b894-4d80-8cab-cac2efc54fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Altair takes in <class 'modin.pandas.dataframe.DataFrame'> with Pandas as backend, since we implement the dataframe interchange protocol\n"
     ]
    }
   ],
   "source": [
    "print(f\"Altair takes in {type(df_transactions_filtered_groupby)} with {df_transactions_filtered_groupby.get_backend()} as backend, since we implement the dataframe interchange protocol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02653381-f6ca-4c87-a654-31df9fbba223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Starting with pandas version 3.0 all arguments of to_dict except for the argument 'orient' will be keyword-only.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-9d394d6af86c4cf0ad3303ee87811ed6.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-9d394d6af86c4cf0ad3303ee87811ed6.vega-embed details,\n",
       "  #altair-viz-9d394d6af86c4cf0ad3303ee87811ed6.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-9d394d6af86c4cf0ad3303ee87811ed6\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-9d394d6af86c4cf0ad3303ee87811ed6\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-9d394d6af86c4cf0ad3303ee87811ed6\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-efcecdca98fdb0ef70defc9471343726\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"Label\", \"legend\": {\"title\": \"Type\"}, \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"DATE\", \"type\": \"temporal\"}, {\"field\": \"Value\", \"type\": \"quantitative\"}, {\"field\": \"Label\", \"type\": \"nominal\"}], \"x\": {\"field\": \"DATE\", \"timeUnit\": \"monthdate\", \"type\": \"temporal\"}, \"y\": {\"field\": \"Value\", \"scale\": {\"domain\": [400, 600]}, \"type\": \"quantitative\"}}, \"title\": \"Revenue vs Predicted Revenue (by Value)\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-efcecdca98fdb0ef70defc9471343726\": [{\"DATE\": \"2025-10-01T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-02T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-03T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-04T00:00:00.000000\", \"Value\": 530.4670767202472, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-05T00:00:00.000000\", \"Value\": 530.4670767202472, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-06T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-07T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-08T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-09T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-10T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-11T00:00:00.000000\", \"Value\": 530.4670767202472, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-12T00:00:00.000000\", \"Value\": 530.4670767202472, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-13T00:00:00.000000\", \"Value\": 479.9464027468902, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-14T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-15T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-16T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-17T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-18T00:00:00.000000\", \"Value\": 530.4670767202472, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-19T00:00:00.000000\", \"Value\": 530.4670767202472, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-20T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-21T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-22T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-23T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-24T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-25T00:00:00.000000\", \"Value\": 530.4670767202472, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-26T00:00:00.000000\", \"Value\": 530.4670767202472, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-27T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-28T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-29T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-30T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2025-10-31T00:00:00.000000\", \"Value\": 505.20673973356867, \"Label\": \"Predicted Revenue\"}, {\"DATE\": \"2024-10-01T00:00:00.000000\", \"Value\": 505.81308932138046, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-02T00:00:00.000000\", \"Value\": 505.3006759928645, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-03T00:00:00.000000\", \"Value\": 508.76839096617, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-04T00:00:00.000000\", \"Value\": 508.8204457319067, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-05T00:00:00.000000\", \"Value\": 506.16166768050164, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-06T00:00:00.000000\", \"Value\": 506.2496961764981, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-07T00:00:00.000000\", \"Value\": 505.925380138915, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-08T00:00:00.000000\", \"Value\": 503.8346555323591, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-09T00:00:00.000000\", \"Value\": 503.96783805514826, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-10T00:00:00.000000\", \"Value\": 503.12213704994195, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-11T00:00:00.000000\", \"Value\": 506.1319575028206, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-12T00:00:00.000000\", \"Value\": 505.0972658631525, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-13T00:00:00.000000\", \"Value\": 504.53779899826804, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-14T00:00:00.000000\", \"Value\": 506.0659561538821, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-15T00:00:00.000000\", \"Value\": 506.82806919901674, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-16T00:00:00.000000\", \"Value\": 506.5092353158478, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-17T00:00:00.000000\", \"Value\": 500.3716166110493, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-18T00:00:00.000000\", \"Value\": 507.62716303432796, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-19T00:00:00.000000\", \"Value\": 504.6290943219146, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-20T00:00:00.000000\", \"Value\": 508.9709729017689, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-21T00:00:00.000000\", \"Value\": 504.1468995715429, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-22T00:00:00.000000\", \"Value\": 504.1691565370697, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-23T00:00:00.000000\", \"Value\": 500.3612314284384, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-24T00:00:00.000000\", \"Value\": 506.1884994421718, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-25T00:00:00.000000\", \"Value\": 507.97925465261983, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-26T00:00:00.000000\", \"Value\": 504.9986349729348, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-27T00:00:00.000000\", \"Value\": 504.7449714659931, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-28T00:00:00.000000\", \"Value\": 502.5385192479017, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-29T00:00:00.000000\", \"Value\": 505.46339406997987, \"Label\": \"Revenue\"}, {\"DATE\": \"2024-10-30T00:00:00.000000\", \"Value\": 501.1709085805184, \"Label\": \"Revenue\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_forecast_labeled = df_forecast.copy()\n",
    "df_forecast_labeled['Label'] = 'Predicted Revenue'\n",
    "df_forecast_labeled = df_forecast_labeled.rename(columns={'PREDICTED_REVENUE': 'Value'})\n",
    "df_last_year_labeled = df_transactions_filtered_groupby.copy()\n",
    "df_last_year_labeled['Label'] = 'Revenue'\n",
    "df_last_year_labeled = df_last_year_labeled.rename(columns={'REVENUE': 'Value'})\n",
    "\n",
    "# Combine\n",
    "combined_df = pd.concat([\n",
    "    df_forecast_labeled[['DATE', 'Value', 'Label']],\n",
    "    df_last_year_labeled[['DATE', 'Value', 'Label']]\n",
    "])\n",
    "\n",
    "# Plot with Value on X and color based on Label\n",
    "final_chart = alt.Chart(combined_df).mark_line().encode(\n",
    "    y=alt.Y('Value:Q',scale=alt.Scale(domain=[400, 600])),\n",
    "    x='monthdate(DATE):T',\n",
    "    color=alt.Color('Label:N', legend=alt.Legend(title='Type')),\n",
    "    tooltip=['DATE', 'Value', 'Label']\n",
    ").properties(\n",
    "    title='Revenue vs Predicted Revenue (by Value)'\n",
    ")\n",
    "\n",
    "final_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13529e7-d375-4beb-a9c1-cf41a13b0bb4",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Dataframe type stays the same independent of choice of backend"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
